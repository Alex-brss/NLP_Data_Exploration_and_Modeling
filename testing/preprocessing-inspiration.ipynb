{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\courn\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\courn\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from spellchecker import SpellChecker\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import FrenchStemmer\n",
    "from unidecode import unidecode\n",
    "import json\n",
    "import random\n",
    "import ast\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# get all compagny url\n",
    "page = 1\n",
    "result_len = 0\n",
    "compagnies_url = []\n",
    "while True:\n",
    "    url = f\"https://fr.trustpilot.com/categories/car_dealer?page={page}\"\n",
    "    response = requests.get(url)\n",
    "    web_page = response.text\n",
    "    soup = BeautifulSoup(web_page, \"html.parser\")\n",
    "\n",
    "    resp = soup.select(\"a[data-business-unit-card-link]\")\n",
    "    for res in resp:\n",
    "        url = res[\"href\"].replace(\"/review/\", \"\")\n",
    "        compagnies_url.append(url)\n",
    "    if result_len == 0:\n",
    "        result_len = len(resp)\n",
    "    elif result_len != len(resp):\n",
    "        break\n",
    "    print(page)\n",
    "    page += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compagny_url(compagny_url):\n",
    "    # Initialize lists\n",
    "    review_titles = []\n",
    "    review_ratings = []\n",
    "    review_texts = []\n",
    "    review_locations = []\n",
    "    page_number = []\n",
    "\n",
    "    # Set Trustpilot page numbers to scrape here\n",
    "    from_page = 1\n",
    "    to_page = 500\n",
    "\n",
    "    for i in range(from_page, to_page + 1):\n",
    "        response = requests.get(f\"https://fr.trustpilot.com/review/{compagny_url}?page={i}\")\n",
    "        web_page = response.text\n",
    "        soup = BeautifulSoup(web_page, \"html.parser\")\n",
    "\n",
    "        if soup.find_all(\"article\") == []:\n",
    "            break\n",
    "\n",
    "        for review in soup.find_all(\"article\"):\n",
    "            # Review titles\n",
    "            review_title = review.select_one(\"a[data-review-title-typography]\")\n",
    "            if review_title == None:\n",
    "                review_titles.append(\"\")\n",
    "            else:\n",
    "                review_titles.append(review_title.getText())\n",
    "\n",
    "            # Review text\n",
    "            review_text = review.select_one(\"p[data-service-review-text-typography]\")\n",
    "            if review_text == None:\n",
    "                review_texts.append(\"\")\n",
    "            else:\n",
    "                review_texts.append(review_text.getText())\n",
    "\n",
    "            # Review ratings\n",
    "            review_rating = review.select_one(\"div[data-service-review-rating]\")\n",
    "            review_ratings.append(review_rating[\"data-service-review-rating\"])\n",
    "                \n",
    "            review_location = review.select_one(\"div[data-consumer-country-typography]\")\n",
    "            if review_location == None:\n",
    "                review_locations.append(\"\")\n",
    "            else:\n",
    "                review_locations.append(review_location.getText())\n",
    "            # Trustpilot page number\n",
    "            page_number.append(i)\n",
    "\n",
    "    # Create final dataframe from lists\n",
    "    return pd.DataFrame(list(zip([compagny_url for i in range(len(review_titles))], review_titles, review_ratings, review_texts, review_locations)),\n",
    "                    columns =['compagny', 'review_title', 'review_rating', 'review_text', 'review_location'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/79 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [09:13<00:00,  7.01s/it] \n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "for compagny in tqdm(compagnies_url):\n",
    "    result = compagny_url(compagny)\n",
    "    df = pd.concat([df, result])\n",
    "\n",
    "df.to_csv(\"reviews.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    useless_words = pd.read_csv(\"most_frequent_words_mixed.csv\", header=None)[0].tolist()[:100]\n",
    "except:\n",
    "    useless_words = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\courn\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFCamembertForSequenceClassification.\n",
      "\n",
      "All the layers of TFCamembertForSequenceClassification were initialized from the model checkpoint at tblard/tf-allocine.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFCamembertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "STEMMER = FrenchStemmer()\n",
    "spell = SpellChecker(language='fr')\n",
    "pipe = pipeline(\"text-classification\", model=\"tblard/tf-allocine\")\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Suppression des accents\n",
    "    text = unidecode(text)\n",
    "    # Suppression du code HTML\n",
    "    text = re.sub(re.compile(\"<.*?>\"), \"\", text)\n",
    "    text = re.sub(r'[^a-zA-Z0-9/s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    # Suppresssion des nombres\n",
    "    text = re.sub(r'[0-9]+', ' ', text)\n",
    "    # Supprimer les lignes vides\n",
    "    text = text.split('\\n')\n",
    "    text = [line.strip() for line in text if len(line) > 0]\n",
    "    text = ' '.join(text)\n",
    "    # Supprimer les liens\n",
    "    text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "    # Lemmatiser les mots\n",
    "    tokens = word_tokenize(text.lower(), language='french')\n",
    "    return tokens\n",
    "\n",
    "n_grams = lambda tokens, n: [\" \".join(tokens[i:i+n]) for i in range(len(tokens)-n+1)]\n",
    "\n",
    "def text2Token(text, spelling = True, stem = True, len_word_min = 2, spell = spell, useless_words = useless_words):\n",
    "    stopword = stopwords.words('french')\n",
    "    word_tokens = preprocess_text(text)\n",
    "    word_tokens = [word for word in word_tokens if word not in stopword and word not in useless_words and len(word) > len_word_min]\n",
    "    if spelling:\n",
    "        word_tokens = [spell.correction(word) for word in word_tokens]\n",
    "        word_tokens = [word for word in word_tokens if word != None]\n",
    "    if stem:\n",
    "        word_tokens = [STEMMER.stem(token) for token in word_tokens]\n",
    "    word_tokens_with_n_grams = word_tokens + n_grams(word_tokens, 2) + n_grams(word_tokens, 3)\n",
    "    return word_tokens_with_n_grams\n",
    "\n",
    "def getMostFrequentWords(documents, top=10):\n",
    "    # Compter les mots\n",
    "    word_count = {}\n",
    "    for doc in documents:\n",
    "        for word in doc:\n",
    "            if word in word_count:\n",
    "                word_count[word] += 1\n",
    "            else:\n",
    "                word_count[word] = 1\n",
    "\n",
    "    # Trier les mots par fréquence décroissante\n",
    "    word_count = sorted(word_count.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    if top == float('inf'):\n",
    "        return word_count\n",
    "    \n",
    "    return word_count[:top]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocess data\n"
     ]
    }
   ],
   "source": [
    "# Preprocess data\n",
    "print(\"Preprocess data\")\n",
    "df = pd.read_csv(\"reviews.csv\")\n",
    "df = df.dropna(subset=['review_text'])\n",
    "df = df[df[\"review_location\"]  == \"FR\"]\n",
    "df = df.drop_duplicates(subset=['review_text'])\n",
    "df = df.reset_index(drop=True)\n",
    "df = df[['review_text', 'review_rating']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get most frequent words list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/16933 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 19/16933 [00:09<2:19:33,  2.02it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m df_prepared_data \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(df))):\n\u001b[1;32m----> 3\u001b[0m     df_prepared_data\u001b[38;5;241m.\u001b[39mappend(\u001b[43mtext2Token\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mat\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreview_text\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43museless_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[1;32mIn[3], line 31\u001b[0m, in \u001b[0;36mtext2Token\u001b[1;34m(text, spelling, stem, len_word_min, spell, useless_words)\u001b[0m\n\u001b[0;32m     29\u001b[0m word_tokens \u001b[38;5;241m=\u001b[39m [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m word_tokens \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stopword \u001b[38;5;129;01mand\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m useless_words \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(word) \u001b[38;5;241m>\u001b[39m len_word_min]\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spelling:\n\u001b[1;32m---> 31\u001b[0m     word_tokens \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mspell\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcorrection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mword\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mword_tokens\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     32\u001b[0m     word_tokens \u001b[38;5;241m=\u001b[39m [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m word_tokens \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stem:\n",
      "Cell \u001b[1;32mIn[3], line 31\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     29\u001b[0m word_tokens \u001b[38;5;241m=\u001b[39m [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m word_tokens \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stopword \u001b[38;5;129;01mand\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m useless_words \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(word) \u001b[38;5;241m>\u001b[39m len_word_min]\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spelling:\n\u001b[1;32m---> 31\u001b[0m     word_tokens \u001b[38;5;241m=\u001b[39m [\u001b[43mspell\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcorrection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m word_tokens]\n\u001b[0;32m     32\u001b[0m     word_tokens \u001b[38;5;241m=\u001b[39m [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m word_tokens \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stem:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\spellchecker\\spellchecker.py:159\u001b[0m, in \u001b[0;36mSpellChecker.correction\u001b[1;34m(self, word)\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"The most probable correct spelling for the word\u001b[39;00m\n\u001b[0;32m    153\u001b[0m \n\u001b[0;32m    154\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;124;03m    word (str): The word to correct\u001b[39;00m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;124;03mReturns:\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;124;03m    str: The most likely candidate or None if no correction is present\"\"\"\u001b[39;00m\n\u001b[0;32m    158\u001b[0m word \u001b[38;5;241m=\u001b[39m ensure_unicode(word)\n\u001b[1;32m--> 159\u001b[0m candidates \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcandidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m candidates:\n\u001b[0;32m    161\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\spellchecker\\spellchecker.py:186\u001b[0m, in \u001b[0;36mSpellChecker.candidates\u001b[1;34m(self, word)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;66;03m# if still not found, use the edit distance 1 to calc edit distance 2\u001b[39;00m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_distance \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m--> 186\u001b[0m     tmp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mknown(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__edit_distance_alt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tmp:\n\u001b[0;32m    188\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m tmp\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\spellchecker\\spellchecker.py:253\u001b[0m, in \u001b[0;36mSpellChecker.__edit_distance_alt\u001b[1;34m(self, words)\u001b[0m\n\u001b[0;32m    251\u001b[0m tmp_words \u001b[38;5;241m=\u001b[39m [ensure_unicode(w) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m words]\n\u001b[0;32m    252\u001b[0m tmp \u001b[38;5;241m=\u001b[39m [w \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_case_sensitive \u001b[38;5;28;01melse\u001b[39;00m w\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m tmp_words \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_if_should_check(w)]\n\u001b[1;32m--> 253\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[43me2\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43me1\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtmp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43me2\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mknown\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medit_distance_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43me1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\spellchecker\\spellchecker.py:253\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    251\u001b[0m tmp_words \u001b[38;5;241m=\u001b[39m [ensure_unicode(w) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m words]\n\u001b[0;32m    252\u001b[0m tmp \u001b[38;5;241m=\u001b[39m [w \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_case_sensitive \u001b[38;5;28;01melse\u001b[39;00m w\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m tmp_words \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_if_should_check(w)]\n\u001b[1;32m--> 253\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [e2 \u001b[38;5;28;01mfor\u001b[39;00m e1 \u001b[38;5;129;01min\u001b[39;00m tmp \u001b[38;5;28;01mfor\u001b[39;00m e2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mknown(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medit_distance_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43me1\u001b[49m\u001b[43m)\u001b[49m)]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\spellchecker\\spellchecker.py:228\u001b[0m, in \u001b[0;36mSpellChecker.edit_distance_1\u001b[1;34m(self, word)\u001b[0m\n\u001b[0;32m    226\u001b[0m deletes \u001b[38;5;241m=\u001b[39m [L \u001b[38;5;241m+\u001b[39m R[\u001b[38;5;241m1\u001b[39m:] \u001b[38;5;28;01mfor\u001b[39;00m L, R \u001b[38;5;129;01min\u001b[39;00m splits \u001b[38;5;28;01mif\u001b[39;00m R]\n\u001b[0;32m    227\u001b[0m transposes \u001b[38;5;241m=\u001b[39m [L \u001b[38;5;241m+\u001b[39m R[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m R[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m R[\u001b[38;5;241m2\u001b[39m:] \u001b[38;5;28;01mfor\u001b[39;00m L, R \u001b[38;5;129;01min\u001b[39;00m splits \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(R) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m--> 228\u001b[0m replaces \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mL\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mR\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mR\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msplits\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mR\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mletters\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    229\u001b[0m inserts \u001b[38;5;241m=\u001b[39m [L \u001b[38;5;241m+\u001b[39m c \u001b[38;5;241m+\u001b[39m R \u001b[38;5;28;01mfor\u001b[39;00m L, R \u001b[38;5;129;01min\u001b[39;00m splits \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m letters]\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mset\u001b[39m(deletes \u001b[38;5;241m+\u001b[39m transposes \u001b[38;5;241m+\u001b[39m replaces \u001b[38;5;241m+\u001b[39m inserts)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\spellchecker\\spellchecker.py:228\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    226\u001b[0m deletes \u001b[38;5;241m=\u001b[39m [L \u001b[38;5;241m+\u001b[39m R[\u001b[38;5;241m1\u001b[39m:] \u001b[38;5;28;01mfor\u001b[39;00m L, R \u001b[38;5;129;01min\u001b[39;00m splits \u001b[38;5;28;01mif\u001b[39;00m R]\n\u001b[0;32m    227\u001b[0m transposes \u001b[38;5;241m=\u001b[39m [L \u001b[38;5;241m+\u001b[39m R[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m R[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m R[\u001b[38;5;241m2\u001b[39m:] \u001b[38;5;28;01mfor\u001b[39;00m L, R \u001b[38;5;129;01min\u001b[39;00m splits \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(R) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m--> 228\u001b[0m replaces \u001b[38;5;241m=\u001b[39m [L \u001b[38;5;241m+\u001b[39m c \u001b[38;5;241m+\u001b[39m R[\u001b[38;5;241m1\u001b[39m:] \u001b[38;5;28;01mfor\u001b[39;00m L, R \u001b[38;5;129;01min\u001b[39;00m splits \u001b[38;5;28;01mif\u001b[39;00m R \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m letters]\n\u001b[0;32m    229\u001b[0m inserts \u001b[38;5;241m=\u001b[39m [L \u001b[38;5;241m+\u001b[39m c \u001b[38;5;241m+\u001b[39m R \u001b[38;5;28;01mfor\u001b[39;00m L, R \u001b[38;5;129;01min\u001b[39;00m splits \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m letters]\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mset\u001b[39m(deletes \u001b[38;5;241m+\u001b[39m transposes \u001b[38;5;241m+\u001b[39m replaces \u001b[38;5;241m+\u001b[39m inserts)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df_prepared_data = []\n",
    "for i in tqdm(range(len(df))):\n",
    "    df_prepared_data.append(text2Token(df.at[i, \"review_text\"], stem=False, useless_words=[]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_frequent_words = getMostFrequentWords(df_prepared_data, top=1000)\n",
    "pd.DataFrame(most_frequent_words, columns=[\"word\", \"count\"]).to_csv(\"most_frequent_words.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traslate most frequent words list to english (for better performance of the pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_frequent_words = pd.read_csv(\"most_frequent_words.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"en\" not in most_frequent_words.columns:\n",
    "    most_frequent_words[\"en\"] = [None for i in range(len(most_frequent_words))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import translators as ts\n",
    "\n",
    "def translate(word):\n",
    "    try:\n",
    "        return ts.tencent(word, to_language=\"en\", from_language=\"fr\")\n",
    "    except Exception as e:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [47:31<00:00,  2.85s/it] \n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(len(most_frequent_words))):\n",
    "    if most_frequent_words.at[i, \"en\"] == None:\n",
    "        most_frequent_words.at[i, \"en\"] = translate(most_frequent_words.at[i, \"word\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_frequent_words.to_csv(\"most_frequent_words.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the sentiment of the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_frequent_words = pd.read_csv(\"most_frequent_words.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"sentiment\" in most_frequent_words.columns:\n",
    "    most_frequent_words[\"sentiment\"] = [None for i in range(len(most_frequent_words))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [04:29<00:00,  3.71it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(len(most_frequent_words))):\n",
    "    if most_frequent_words.at[i, \"sentiment\"] == None and str(most_frequent_words.at[i, \"word\"]) != \"nan\":\n",
    "        pipe_res = pipe(most_frequent_words.at[i, \"word\"])[0]\n",
    "        most_frequent_words.at[i, \"sentiment\"] = pipe_res[\"label\"] if pipe_res[\"score\"] > 0.65 else \"MIXED\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_frequent_words.to_csv(\"most_frequent_words.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_frequent_words[most_frequent_words[\"sentiment\"] == \"MIXED\"][[\"word\"]].to_csv(\"most_frequent_words_mixed.csv\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "useless_words = most_frequent_words[most_frequent_words[\"sentiment\"] == \"MIXED\"][\"word\"].tolist()[100:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate train dataset\n",
      "Spell check for train dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1905 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1905/1905 [17:02<00:00,  1.86it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save train dataset\n",
      "Generate test dataset\n",
      "Save test dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate train dataset\n",
    "print(\"Generate train dataset\")\n",
    "train = df.copy()\n",
    "rus = RandomUnderSampler(random_state=0)\n",
    "X = train[[\"review_text\"]]\n",
    "y = train[\"review_rating\"]\n",
    "X_resampled, y_resampled = rus.fit_resample(X, y)\n",
    "train = pd.concat([X_resampled, y_resampled], axis=1)\n",
    "train_idx = train.index.tolist()\n",
    "train = train.reset_index(drop=True)\n",
    "\n",
    "# Spell check for train dataset\n",
    "print(\"Spell check for train dataset\")\n",
    "for i in tqdm(range(len(train))):\n",
    "    train.at[i, \"review_text\"] = json.dumps(text2Token(train.at[i, \"review_text\"]))\n",
    "\n",
    "# Save train dataset\n",
    "print(\"Save train dataset\")\n",
    "train.to_csv(\"reviews_train.csv\", index=False)\n",
    "\n",
    "# Generate test dataset\n",
    "print(\"Generate test dataset\")\n",
    "test = df.copy()\n",
    "test = test.drop(train_idx)\n",
    "rus = RandomUnderSampler(random_state=0)\n",
    "X = test[[\"review_text\"]]\n",
    "y = test[\"review_rating\"]\n",
    "X_resampled, y_resampled = rus.fit_resample(X, y)\n",
    "test = pd.concat([X_resampled, y_resampled], axis=1)\n",
    "\n",
    "# Save test dataset\n",
    "print(\"Save test dataset\")\n",
    "test.to_csv(\"reviews_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"reviews_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"review_text\"] = train[\"review_text\"].apply(lambda x: ast.literal_eval(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "review_rating\n",
       "1    381\n",
       "2    381\n",
       "3    381\n",
       "4    381\n",
       "5    381\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"review_rating\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "# Liste d'avis\n",
    "documents = train[\"review_text\"].tolist()\n",
    "\n",
    "# Listes de scores\n",
    "ratings = train[\"review_rating\"].tolist()\n",
    "\n",
    "# Créer un modèle BM25\n",
    "bm25 = BM25Okapi(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTopDocs(bm25, query, documents, ratings, top=5):\n",
    "    # Calculer les scores de similarité\n",
    "    scores = bm25.get_scores(query)\n",
    "\n",
    "    # Associer chaque avis à son score\n",
    "    doc_scores = list(zip(documents, scores, ratings))\n",
    "\n",
    "    # Trier les avis par score décroissant\n",
    "    return sorted(doc_scores, key=lambda x: x[1], reverse=True)[:top]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Not used\n",
    "# Obtenir les mots positifs et négatifs en utilisant bert\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "\n",
    "def getSentiment(text):\n",
    "    # Tokenize the text\n",
    "    tokens = tokenizer.encode(text, return_tensors='pt')\n",
    "    # Reduce the tokens size to 512\n",
    "    tokens = tokens[:, :512]\n",
    "    # Get the prediction\n",
    "    result = model(tokens)\n",
    "    # Return the label\n",
    "    return torch.argmax(result.logits).item()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Not used\n",
    "def getRevelantWords(most_freq, pos_nb = 5, neg_nb = 5):\n",
    "    pos_words = []\n",
    "    neg_words = []\n",
    "    for word, nb in most_freq:\n",
    "        if type(word) == tuple:\n",
    "            word = \" \".join(word)\n",
    "        if getSentiment(word) == 0 and len(neg_words) < neg_nb:\n",
    "            neg_words.append(word)\n",
    "        elif getSentiment(word) == 4 and len(pos_words) < pos_nb:\n",
    "            pos_words.append(word)\n",
    "\n",
    "        if len(neg_words) == neg_nb and len(pos_words) == pos_nb:\n",
    "            break\n",
    "\n",
    "    return pos_words, neg_words\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separer_phrase(phrase):\n",
    "    # On ajoute des points aux sauts de ligne\n",
    "    phrase = phrase.replace('\\n', '.')\n",
    "\n",
    "    # Divise d'abord la phrase en utilisant les points, points d'interrogation, points d'exclamation.\n",
    "    pattern = r'(?<=[.!?])(?=\\s|[A-Z\"\\'(])'\n",
    "    groupes = re.split(pattern, phrase)\n",
    "\n",
    "    groupes_fins = []\n",
    "    for groupe in groupes:\n",
    "        # Combinaison des motifs de virgule et \"et\" en une seule expression régulière\n",
    "        # Sépare sur les virgules (en évitant les nombres décimaux), sur les ; et : et sur les conjonctions de coordinations (avec un contexte spécifique)\n",
    "        pattern_combined = r'(?<=.{20},)\\s(?!\\d)|[;:]|\\b(mais|ou|et|donc|or|ni|car)\\b(?=.{15,})'\n",
    "        sous_groupes = re.split(pattern_combined, groupe)\n",
    "        groupes_fins.extend(sous_groupes)\n",
    "\n",
    "    return [groupe.strip() for groupe in groupes_fins if groupe is not None and groupe.strip() and groupe.strip() not in ['.', ',', 'mais', 'ou', 'et', 'donc', 'or', 'ni', 'car']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_score(top_docs, origin_query = None, use_bm25 = True, FIABILITY_THRESHOLD = 0.6):\n",
    "    # Calculer la note moyenne des avis\n",
    "    stars_bm25 = None\n",
    "    if use_bm25:\n",
    "        stars_bm25 = sum([int(doc[2]) for doc in top_docs]) / len(top_docs)\n",
    "        if origin_query is None:\n",
    "            return stars_bm25\n",
    "    try:\n",
    "        score_pipe = pipe(origin_query)\n",
    "        stars_pipe = 2.5 + 2.5 * FIABILITY_THRESHOLD if score_pipe[0][\"label\"] == \"POSITIVE\" else (2.5 - 2.5 * FIABILITY_THRESHOLD if score_pipe[0][\"label\"] == \"NEGATIVE\" else 2.5)\n",
    "        return (stars_bm25 + stars_pipe) / 2\n",
    "    except:\n",
    "        return stars_bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRevelantSentences(origin_query, most_freq, documents, ratings, top=5, use_bm25 = True, use_pipe = True):\n",
    "\n",
    "    # Appel de la fonction\n",
    "    groupes = separer_phrase(origin_query)\n",
    "\n",
    "    # Obtenir les scores de chaque groupe\n",
    "    scores = []\n",
    "    for groupe in groupes:\n",
    "        if use_pipe and use_bm25:\n",
    "            scores.append(estimate_score(getTopDocs(bm25, text2Token(groupe), documents, ratings), origin_query))\n",
    "        elif use_pipe:\n",
    "            scores.append(estimate_score(None, origin_query, use_bm25=False))\n",
    "        elif use_bm25:\n",
    "            scores.append(estimate_score(getTopDocs(bm25, text2Token(groupe), documents, ratings)))\n",
    "            \n",
    "    pos_list = []\n",
    "    neg_list = []\n",
    "    for groupe, score in zip(groupes, scores):\n",
    "        group_tokens = text2Token(groupe)\n",
    "        sumFreq = sum([freq for word, freq in most_freq if word in group_tokens])\n",
    "        if score >= 3.5:\n",
    "            pos_list.append((groupe, sumFreq))\n",
    "        elif score <= 1.5:\n",
    "            neg_list.append((groupe, sumFreq))\n",
    "\n",
    "    pos_list = [sentence[0] for sentence in sorted(pos_list, key=lambda x: x[1], reverse=True)[:top]]\n",
    "    neg_list = [sentence[0] for sentence in sorted(neg_list, key=lambda x: x[1], reverse=True)[:top]]\n",
    "\n",
    "    return pos_list, neg_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(origin_query, bm25=bm25, documents=documents, ratings=ratings, spell=spell, use_bm25 = True, use_pipe = True):\n",
    "    query = text2Token(origin_query)\n",
    "    if use_bm25:\n",
    "        top_docs = getTopDocs(bm25, query, documents, ratings, top=5)\n",
    "    most_freq = getMostFrequentWords([doc[0] for doc in top_docs], top=50)\n",
    "    \"\"\"  Not used\n",
    "    relevants = getRevelantWords(most_freq, pos_nb = 5, neg_nb = 5)\n",
    "    \"\"\"\n",
    "\n",
    "    pos_list, neg_list = getRevelantSentences(origin_query, most_freq, documents, ratings, top=5, use_bm25 = True, use_pipe = True)\n",
    "    if not use_pipe:\n",
    "        origin_query = None\n",
    "    return estimate_score(top_docs, origin_query, use_bm25=use_bm25), pos_list, neg_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use data unsed and balanced the new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "review_rating\n",
       "1    152\n",
       "3    152\n",
       "4    152\n",
       "5    152\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv(\"reviews_test.csv\")\n",
    "\n",
    "test[\"review_rating\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score_only(origin_query):\n",
    "    query = text2Token(origin_query)\n",
    "    top_docs = getTopDocs(bm25, query, documents, ratings, top=5)\n",
    "    return estimate_score(top_docs, origin_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/608 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 608/608 [09:06<00:00,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.7905030231971882\n",
      "MSE: 1.128923385959617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = test[\"review_text\"], test[\"review_rating\"]\n",
    "\n",
    "y_pred = [get_score_only(query) for query in tqdm(X_test, total=len(X_test))]\n",
    "print(f\"MAE: {mean_absolute_error(y_test, y_pred)}\")\n",
    "print(f\"MSE: {mean_squared_error(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Voiture achetée le 19 août, tombée en panne 1 semaine après (voyant défaut moteur rouge). Restée 2 semaines sur place pour la récupérer et retombée en panne au bout de 10 km. Véhicule immobilisé à nouveau 1 semaine chez eux. Pour retomber en panne au bout de 500km. Envoyée chez le concessionnaire Nissan car plus confiance aux compétences des mécaniciens. En espérant ne plus retomber en panne. Malgré tout, heureusement que le service commercial rattrape un peu les choses...\n",
      "Vrai score: 1\n",
      "Score: 1.7\n",
      "Séquences positives: []\n",
      "Séquences négatives: ['retombée en panne au bout de 10 km.', 'Pour retomber en panne au bout de 500km.', 'En espérant ne plus retomber en panne.', 'Véhicule immobilisé à nouveau 1 semaine chez eux.']\n"
     ]
    }
   ],
   "source": [
    "elmt = test.iloc[random.randint(0, len(test))]\n",
    "origin_query = elmt[\"review_text\"]\n",
    "score, pos_list, neg_list = main(origin_query)\n",
    "\n",
    "print(f\"Query: {origin_query}\")\n",
    "print(f\"Vrai score: {elmt['review_rating']}\")\n",
    "\n",
    "print(f\"Score: {score}\")\n",
    "print(f\"Séquences positives: {pos_list}\")\n",
    "print(f\"Séquences négatives: {neg_list}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "\n",
    "# Importez ici vos fonctions de prédiction (prediction_1, prediction_2, prediction_3)\n",
    "\n",
    "def prediction_1(origin_query):\n",
    "    score, pos_list, neg_list = main(origin_query, use_pipe=False)\n",
    "    return {\n",
    "        \"nombre d étoile sur 5\": score,\n",
    "        \"liste phrases positives\": pos_list,\n",
    "        \"liste phrases négatives\": neg_list\n",
    "    }\n",
    "\n",
    "def prediction_2(origin_query):\n",
    "    score, pos_list, neg_list = main(origin_query, use_bm25=False)\n",
    "    return {\n",
    "        \"nombre d étoile sur 5\": score,\n",
    "        \"liste phrases positives\": pos_list,\n",
    "        \"liste phrases négatives\": neg_list\n",
    "    }\n",
    "\n",
    "def prediction_3(origin_query):\n",
    "    score, pos_list, neg_list = main(origin_query)\n",
    "    return {\n",
    "        \"nombre d étoile sur 5\": score,\n",
    "        \"liste phrases positives\": pos_list,\n",
    "        \"liste phrases négatives\": neg_list\n",
    "    }\n",
    "\n",
    "def afficher_resultats(resultats):\n",
    "    st.subheader(\"Résultats de la prédiction :\")\n",
    "    st.write(f\"Nombre d'étoiles sur 5 : {'🌟' * resultats['nombre d étoile sur 5']}\")\n",
    "    st.subheader(\"Liste de phrases positives :\")\n",
    "    for phrase in resultats[\"liste phrases positives\"]:\n",
    "        st.write(f\"👍 {phrase}\")\n",
    "    st.subheader(\"Liste de phrases négatives :\")\n",
    "    for phrase in resultats[\"liste phrases négatives\"]:\n",
    "        st.write(f\"👎 {phrase}\")\n",
    "\n",
    "def run():\n",
    "    st.title(\"Analyse d'avis Internet\")\n",
    "\n",
    "    avis_utilisateur = st.text_area(\"Entrez votre avis ici :\")\n",
    "\n",
    "    if st.button(\"Prédiction BM 25\"):\n",
    "        resultats_1 = prediction_1(avis_utilisateur)\n",
    "        afficher_resultats(resultats_1)\n",
    "\n",
    "    if st.button(\"Prédiction Transformers\"):\n",
    "        resultats_2 = prediction_2(avis_utilisateur)\n",
    "        afficher_resultats(resultats_2)\n",
    "\n",
    "    if st.button(\"Prédiction BM 25 + Transformers\"):\n",
    "        resultats_3 = prediction_3(avis_utilisateur)\n",
    "        afficher_resultats(resultats_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
