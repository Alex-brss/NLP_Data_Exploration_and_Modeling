{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexandrecogordan/miniconda3/envs/tensorflow/lib/python3.10/site-packages/tensorflow_hub/__init__.py:61: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  from pkg_resources import parse_version\n",
      "/Users/alexandrecogordan/miniconda3/envs/tensorflow/lib/python3.10/site-packages/huggingface_hub/inference/_text_generation.py:121: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.5/migration/\n",
      "  @validator(\"best_of\")\n",
      "/Users/alexandrecogordan/miniconda3/envs/tensorflow/lib/python3.10/site-packages/huggingface_hub/inference/_text_generation.py:140: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.5/migration/\n",
      "  @validator(\"repetition_penalty\")\n",
      "/Users/alexandrecogordan/miniconda3/envs/tensorflow/lib/python3.10/site-packages/huggingface_hub/inference/_text_generation.py:146: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.5/migration/\n",
      "  @validator(\"seed\")\n",
      "/Users/alexandrecogordan/miniconda3/envs/tensorflow/lib/python3.10/site-packages/huggingface_hub/inference/_text_generation.py:152: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.5/migration/\n",
      "  @validator(\"temperature\")\n",
      "/Users/alexandrecogordan/miniconda3/envs/tensorflow/lib/python3.10/site-packages/huggingface_hub/inference/_text_generation.py:158: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.5/migration/\n",
      "  @validator(\"top_k\")\n",
      "/Users/alexandrecogordan/miniconda3/envs/tensorflow/lib/python3.10/site-packages/huggingface_hub/inference/_text_generation.py:164: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.5/migration/\n",
      "  @validator(\"top_p\")\n",
      "/Users/alexandrecogordan/miniconda3/envs/tensorflow/lib/python3.10/site-packages/huggingface_hub/inference/_text_generation.py:170: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.5/migration/\n",
      "  @validator(\"truncate\")\n",
      "/Users/alexandrecogordan/miniconda3/envs/tensorflow/lib/python3.10/site-packages/huggingface_hub/inference/_text_generation.py:176: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.5/migration/\n",
      "  @validator(\"typical_p\")\n",
      "/Users/alexandrecogordan/miniconda3/envs/tensorflow/lib/python3.10/site-packages/huggingface_hub/inference/_text_generation.py:204: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.5/migration/\n",
      "  @validator(\"inputs\")\n",
      "/Users/alexandrecogordan/miniconda3/envs/tensorflow/lib/python3.10/site-packages/huggingface_hub/inference/_text_generation.py:210: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.5/migration/\n",
      "  @validator(\"stream\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports\n",
    "\n",
    "import nltk\n",
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "import spacy\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import matplotlib.pyplot as plt\n",
    "import streamlit as st\n",
    "import torch\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gensim.downloader as api\n",
    "import tensorflow_hub as hub\n",
    "import nlpaug.augmenter.word as naw\n",
    "import random\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from textblob import TextBlob\n",
    "from collections import Counter\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification, AutoModel, AutoTokenizer\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel, Word2Vec, KeyedVectors\n",
    "from sklearn.manifold import TSNE\n",
    "from PIL import Image\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tensorboard.plugins import projector\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from tensorboard.plugins import projector\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from scipy.spatial.distance import euclidean, cosine\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from nlpaug.util import Action\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Functions ---------- #\n",
    "\n",
    "# ---------- Loading the dataset ---------- #\n",
    "\n",
    "df = pd.read_csv('yelp_reviews.csv')\n",
    "\n",
    "df.drop_duplicates(inplace=True)\n",
    "df.dropna(subset=['text', 'rating', 'location'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Preprocessing ---------- #\n",
    "\n",
    "# Translation pipeline\n",
    "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-zh-en\")\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['text'] = df['text'].astype(str)\n",
    "\n",
    "# Check if text contains Chinese characters\n",
    "def contains_chinese(text):\n",
    "    return bool(re.search('[\\u4e00-\\u9fff]', text))\n",
    "\n",
    "# Translation function (from Chinese to English)\n",
    "def translate_text(text):\n",
    "    if contains_chinese(text):\n",
    "        return translator(text)[0]['translation_text']\n",
    "    else:\n",
    "        return text\n",
    "    \n",
    "# Lemmatisation & Tokenisation function\n",
    "def tokenisation(reviews, allowed_postags=[\"NOUN\", \"ADJ\", \"VERBS\", \"ADV\"]):\n",
    "    nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "    reviews_out = []\n",
    "    tokens = []\n",
    "\n",
    "    for review in reviews:\n",
    "        doc = nlp(review) \n",
    "        reviews_out.append(\" \".join([token.lemma_ for token in doc if token.pos_ in allowed_postags and token.lemma_ not in stop_words]))\n",
    "    \n",
    "    for text in reviews_out:\n",
    "        new = gensim.utils.simple_preprocess(text, deacc=False) # We do not remove the accent marks because we deem them important for French restaurants reviews\n",
    "        tokens.append(new)\n",
    "\n",
    "    return tokens\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocessing(text):\n",
    "    # Corrected spelling on lower case text\n",
    "    corrected_text = str(TextBlob(text.lower()).correct())\n",
    "\n",
    "    # Translation\n",
    "    cleaned_text = translate_text(str(corrected_text))\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "# Apply preprocessing and tokenisation\n",
    "df['cleaned_text'] = df['text'].apply(preprocessing)\n",
    "df['tokens'] = tokenisation(df['cleaned_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarisation\n",
    "\n",
    "max_length_coef = 1.5\n",
    "min_length_coef = 2\n",
    "\n",
    "summariser = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "summarised_text = df['text'].apply(lambda x: summariser(x, max_length=round(len(x)/max_length_coef), min_length=round(len(x)/min_length_coef), do_sample=False))\n",
    "df['summarised_text'] = summarised_text.apply(lambda x: x[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Highlighting frequent words ---------- #\n",
    "\n",
    "review_frequent_words = {}\n",
    "\n",
    "def get_frequency(restaurant_id):\n",
    "\n",
    "    # Word Frequency Analysis\n",
    "    all_words = [word for tokens in df[df['restaurant_id'] == restaurant_id]['tokens'] for word in tokens]\n",
    "    word_freq = Counter(all_words)\n",
    "\n",
    "    # N-gram Analysis\n",
    "    bigrams = ngrams(all_words, 2)\n",
    "    bigram_freq = Counter(bigrams)\n",
    "\n",
    "    # Tri-gram Analysis\n",
    "    trigrams = ngrams(all_words, 3)\n",
    "    trigram_freq = Counter(trigrams)\n",
    "\n",
    "    return [word_freq, bigram_freq, trigram_freq]\n",
    "\n",
    "for restaurant_id in df['restaurant_id']:\n",
    "    review_frequent_words[restaurant_id] = get_frequency(restaurant_id)\n",
    "\n",
    "review_frequent_words_df = pd.DataFrame.from_dict(review_frequent_words, orient='index', columns=['word_freq', 'bigram_freq', 'trigram_freq'])\n",
    "review_frequent_words_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexandrecogordan/miniconda3/envs/tensorflow/lib/python3.10/site-packages/sklearn/manifold/_mds.py:298: FutureWarning: The default value of `normalized_stress` will change to `'auto'` in version 1.4. To suppress this warning, manually set the value of `normalized_stress`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el15613117161316641008217947\" style=\"background-color:white;\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el15613117161316641008217947_data = {\"mdsDat\": {\"x\": [0.39425905455194066, 0.09009589925450281, -0.23664328063543053, -0.24304431291986833, 0.09828469778129069, -0.05284493692450437, -0.0930052796017752, -0.2728443060560079, 0.2223149033644846, 0.09342756118536749], \"y\": [0.149253718724891, 0.30300515201646344, 0.02605960347576295, 0.24535274375807586, -0.33371467355171025, 0.18013771696442438, -0.25200066799470394, -0.15465084833732679, -0.1568784023824518, -0.006564342673424759], \"topics\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \"cluster\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \"Freq\": [38.37367098418325, 12.396911413431784, 8.166624833927779, 6.893577819714761, 6.886240894601893, 6.783201540678463, 6.511887628272792, 5.85787973881224, 4.671009403313952, 3.458995743063086]}, \"tinfo\": {\"Term\": [\"service\", \"food\", \"birthday\", \"restaurant\", \"time\", \"experience\", \"atmosphere\", \"course\", \"good\", \"table\", \"service\", \"food\", \"great\", \"alliance\", \"place\", \"nice\", \"wine\", \"wonderful\", \"delicious\", \"well\", \"good\", \"night\", \"restaurant\", \"amazing\", \"meal\", \"little\", \"dish\", \"special\", \"many\", \"day\", \"waiter\", \"fresh\", \"rude\", \"martin\", \"reservation\", \"dinner\", \"french\", \"experience\", \"parking\", \"option\", \"potato\", \"small\", \"location\", \"olive\", \"classic\", \"dine\", \"melon\", \"prosciutto\", \"speak\", \"dining\", \"menu\", \"soup\", \"onion\", \"experience\", \"french\", \"course\", \"thing\", \"life\", \"party\", \"urinary\", \"glass\", \"etouffee\", \"profiterole\", \"third\", \"journey\", \"table\", \"server\", \"birthday\", \"family\", \"warm\", \"super\", \"week\", \"excited\", \"right\", \"copy\", \"celebration\", \"close\", \"fun\", \"time\", \"really\", \"overall\", \"kiss\", \"attentive\", \"fantastic\", \"town\", \"upside\", \"downside\", \"pithivier\", \"city\", \"bread\", \"experience\", \"table\", \"branch\", \"year\", \"last\", \"second\", \"tonight\", \"truly\", \"brasserie\", \"new\", \"son\", \"sick\", \"busy\", \"still\", \"star\", \"time\", \"first\", \"favorite\", \"area\", \"bit\", \"high\", \"mummy\", \"disappointed\", \"kid\", \"extremely\", \"ever\", \"atmosphere\", \"selection\", \"lovely\", \"escargot\", \"surprised\", \"event\", \"hard\", \"interesting\", \"overdue\", \"salmon\", \"anywhere\", \"cave\", \"creek\", \"favorite\", \"back\", \"way\", \"incredible\", \"class\", \"wall\", \"mood\", \"shelf\", \"lighting\", \"pass\", \"enough\", \"moment\", \"highly\", \"server\"], \"Freq\": [173.0, 169.0, 34.0, 131.0, 60.0, 53.0, 30.0, 27.0, 100.0, 32.0, 172.98332675803064, 168.79411340870442, 79.38500081310895, 63.713413007964625, 64.01137865373067, 42.80972989362263, 41.695085238100326, 37.22107358901608, 36.67443735662864, 26.643497735517162, 94.91347676872395, 52.06349254300194, 118.12247928890199, 54.40303033081273, 26.59803410271131, 16.6881974658797, 15.832407882768337, 14.70171652025367, 14.47652593987731, 14.215190029322457, 13.340330060856617, 12.621563570011244, 12.180868254853305, 10.738250026568558, 17.12754401104946, 26.985644161990948, 20.503039171394175, 15.774021349697295, 12.454412611481274, 11.420432362061748, 9.767685768691939, 9.161217322148598, 8.750987178224628, 8.546544347628322, 8.134566188899726, 7.880562878566908, 7.880562878566908, 7.880562878566908, 21.129860645382376, 20.627885471621614, 18.95456997774681, 14.037951113612122, 13.354809700608993, 14.9705671022882, 10.053440780186662, 26.80310824255389, 14.113172300886838, 13.221343818338747, 12.47696451178629, 9.078262151050613, 8.833006947390468, 6.884495593198123, 6.884495593198123, 6.884495593198123, 6.884075143061017, 21.17329197118552, 9.510868197101493, 33.7526758402789, 16.113758378130612, 13.39132174982685, 12.087345375315287, 11.56706946537342, 11.3475454505296, 10.750340371631072, 8.35980688734721, 7.981855821591169, 7.62547726018886, 8.602164203962984, 16.92316663626922, 14.648658093608114, 18.995535942126697, 17.000414552194307, 15.014404292768665, 12.838043262056148, 10.504870623294629, 8.45974144510377, 7.0456194900896865, 7.045351195447759, 6.903959141486136, 6.377649819577166, 22.74758440589101, 11.004532454767944, 7.501974678027698, 21.267948930361037, 18.97911947392199, 11.223199678754806, 10.123695051539366, 9.43632697810078, 9.369880842724985, 8.43798579504152, 7.72604691828745, 7.490691441330206, 5.558899751523678, 8.875236648269425, 11.14042924873583, 15.134035120054998, 11.4666553385017, 10.191543227770365, 16.953492584169737, 14.13390927539523, 13.246982663155427, 11.59223842274008, 8.498383971991759, 7.463705430592288, 5.846958592065638, 5.468598383191893, 27.452682643001033, 4.459015158342307, 9.599888216831284, 14.145516731887136, 8.304562082398968, 7.5509884788063, 6.950859848540039, 6.453569804950134, 6.1314552620236205, 6.023766010801109, 5.704405068790507, 5.704405068790507, 5.704405068790507, 16.38477048706107, 6.355029234794801, 11.356390312585077, 9.598523543150376, 8.718318789107029, 6.977724559398834, 6.533668088943122, 6.2059335840695695, 5.771429832170928, 5.770974185459895, 5.546784106850667, 2.8781541152216206, 3.6514825512111773, 3.994794304523449], \"Total\": [173.0, 169.0, 34.0, 131.0, 60.0, 53.0, 30.0, 27.0, 100.0, 32.0, 173.6280451739908, 169.4390139344442, 80.02972414525661, 64.35809991080254, 64.68226773864185, 43.45467666467171, 42.339752775615494, 37.865723233652744, 37.31916913480532, 27.28819136915734, 100.35631261555343, 54.06897312168821, 131.49340583527186, 58.958719591393866, 27.486952292368326, 17.333205992866084, 16.480982413183373, 15.34674755500077, 15.121787464596462, 14.860206098946534, 13.985554929810293, 13.267176721606601, 12.826219729682956, 11.383440283485191, 22.2796594658082, 73.76317660407064, 71.46806880147228, 53.98792752581204, 13.11575272094649, 12.081538670132328, 10.42896540049282, 9.822307970008676, 9.4120653994297, 9.207957161696008, 8.795621782705298, 8.541661469074276, 8.541661469074276, 8.541661469074276, 24.121768943888387, 33.29020468322175, 32.782989036770196, 23.187885402569098, 23.040263117619688, 53.98792752581204, 71.46806880147228, 27.474115364140342, 14.784312565651899, 13.892432292233531, 13.148153864411977, 9.749301070159564, 9.504028224601488, 7.555520263685204, 7.555520263685204, 7.555520263685204, 7.555199069308632, 32.77850348395657, 19.337115638650836, 34.42522925337534, 16.786271407406083, 14.063871547490143, 12.759843677580387, 12.23960918440296, 12.020100913913932, 11.42312698250675, 9.03233273262058, 8.654352854284898, 8.29798209585519, 10.780928302572201, 60.062164876883536, 50.546246959477514, 19.665034835040323, 17.66988191433738, 15.683817209210568, 13.507554083568456, 11.174452405309447, 9.129157704085593, 7.715163482242422, 7.7148949644156835, 7.57339890946754, 7.0470657859199, 53.98792752581204, 32.77850348395657, 17.4573439630346, 21.93293759897039, 19.64407222185938, 11.88828268363836, 10.788889190335437, 10.101310706885668, 10.03485445275163, 9.102941873248271, 8.391669062403261, 8.155732533688568, 6.223902052285361, 10.806544202116724, 16.813672695843042, 60.062164876883536, 39.869767086994116, 27.184341279731854, 17.618776631123314, 14.799221596210376, 13.91220474496928, 12.257601075904162, 9.163990088873193, 8.129245099401482, 6.512174157216205, 6.133843904105462, 30.872485618099766, 5.124309696896033, 22.06681792512914, 14.828554798664847, 8.987363850117196, 8.233910196412056, 7.633906352623251, 7.1368090968275935, 6.8147686680369475, 6.707015816234534, 6.387178030918561, 6.387178030918561, 6.387178030918561, 27.184341279731854, 10.1424514180617, 12.038592369771875, 10.28084819254, 9.400652399239835, 7.659866592042915, 7.215886924440719, 6.888108126136962, 6.453654481477783, 6.453294771395485, 6.2289480355398394, 3.5602990186699164, 6.920978527232652, 19.337115638650836], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\"], \"logprob\": [10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -2.614, -2.6385, -3.3929, -3.6128, -3.6082, -4.0105, -4.0368, -4.1503, -4.1651, -4.4847, -3.2143, -3.8148, -2.9955, -3.7708, -3.3565, -3.8226, -3.8752, -3.9493, -3.9648, -3.983, -4.0465, -4.1019, -4.1374, -4.2635, -3.7966, -3.342, -3.6167, -3.8789, -3.6978, -3.7845, -3.9408, -4.0049, -4.0507, -4.0744, -4.1238, -4.1555, -4.1555, -4.1555, -3.1692, -3.1933, -3.2779, -3.5781, -3.628, -3.5138, -3.912, -2.7619, -3.4033, -3.4686, -3.5266, -3.8446, -3.8719, -4.1212, -4.1212, -4.1212, -4.1212, -2.9977, -3.798, -2.5303, -3.2697, -3.4548, -3.5572, -3.6012, -3.6204, -3.6744, -3.9259, -3.9722, -4.0179, -3.8974, -3.2207, -3.365, -3.0901, -3.2011, -3.3253, -3.4819, -3.6825, -3.899, -4.0819, -4.0819, -4.1022, -4.1815, -2.9098, -3.636, -4.0191, -2.9363, -3.0501, -3.5755, -3.6786, -3.7489, -3.756, -3.8607, -3.9489, -3.9798, -4.2781, -3.8102, -3.5829, -3.2765, -3.554, -3.6719, -3.0572, -3.2391, -3.3039, -3.4373, -3.7478, -3.8776, -4.1217, -4.1886, -2.5752, -4.3927, -3.6259, -3.0118, -3.5444, -3.6395, -3.7224, -3.7966, -3.8478, -3.8655, -3.92, -3.92, -3.92, -2.8649, -3.812, -2.931, -3.0992, -3.1954, -3.4181, -3.4839, -3.5353, -3.6079, -3.608, -3.6476, -4.3037, -4.0657, -3.9758], \"loglift\": [10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.9541, 0.954, 0.9497, 0.9477, 0.9474, 0.9428, 0.9425, 0.9406, 0.9404, 0.9339, 0.902, 0.92, 0.8506, 0.8774, 2.0548, 2.0498, 2.0476, 2.0448, 2.0441, 2.0433, 2.0405, 2.0378, 2.0361, 2.0294, 1.8247, 1.0822, 0.839, 0.8573, 2.4534, 2.4488, 2.4396, 2.4354, 2.4323, 2.4306, 2.427, 2.4246, 2.4246, 2.4246, 2.3727, 2.0265, 1.9572, 2.0032, 1.9597, 1.2224, 0.5438, 2.6499, 2.6281, 2.6251, 2.6222, 2.6033, 2.6014, 2.5816, 2.5816, 2.5816, 2.5816, 2.2375, 1.965, 2.6559, 2.6348, 2.6266, 2.6215, 2.6191, 2.6181, 2.6149, 2.5983, 2.5948, 2.5911, 2.4499, 1.4089, 1.4371, 2.6561, 2.6521, 2.6471, 2.6399, 2.6289, 2.6146, 2.5999, 2.5999, 2.5982, 2.5909, 1.8264, 1.5993, 1.8461, 2.7008, 2.6971, 2.674, 2.6679, 2.6634, 2.663, 2.6557, 2.6489, 2.6465, 2.6185, 2.5347, 2.3199, 1.3531, 1.4854, 1.7505, 2.7989, 2.7914, 2.7884, 2.7816, 2.762, 2.752, 2.7296, 2.7226, 2.72, 2.6983, 2.0051, 3.0166, 2.9848, 2.9772, 2.9701, 2.9632, 2.9581, 2.9564, 2.9507, 2.9507, 2.9507, 2.5575, 2.5963, 3.3059, 3.2955, 3.2888, 3.2709, 3.2649, 3.2599, 3.2525, 3.2524, 3.2482, 3.1515, 2.7248, 1.7872]}, \"token.table\": {\"Topic\": [1, 1, 6, 9, 8, 3, 8, 6, 4, 9, 5, 8, 2, 6, 7, 6, 7, 9, 5, 6, 10, 3, 5, 5, 4, 9, 2, 1, 3, 2, 3, 1, 2, 3, 4, 6, 8, 2, 6, 10, 9, 4, 9, 8, 5, 2, 3, 6, 8, 5, 6, 7, 9, 1, 7, 1, 1, 2, 3, 2, 5, 9, 4, 1, 3, 1, 9, 8, 8, 10, 10, 9, 4, 8, 6, 7, 4, 10, 2, 3, 2, 8, 2, 2, 2, 3, 2, 3, 9, 10, 10, 8, 7, 1, 1, 6, 3, 2, 3, 3, 6, 9, 3, 4, 10, 6, 1, 3, 4, 3, 1, 2, 5, 2, 4, 1, 3, 7, 5, 2, 9, 7, 8, 4, 6, 10, 1, 10, 7, 3, 7, 2, 3, 3, 8, 2, 7, 8, 3, 7, 5, 9, 4, 6, 4, 4, 1, 5, 7, 7, 6, 7, 6, 4, 2, 10, 5, 10, 5, 1, 1, 1, 7], \"Freq\": [0.9944358221995546, 0.9158950597000807, 0.06784407849630227, 0.9393819885645368, 0.9648797050965359, 0.09717390549989195, 0.8745651494990275, 0.956399822818071, 0.29578647965299526, 0.5915729593059905, 0.9876477437449847, 0.9459957004485268, 0.5155423424695778, 0.45825985997295804, 0.8968739947725037, 0.8514181905308807, 0.9640254537419743, 0.9393819885645368, 0.9243903194955914, 0.9242877714059494, 0.9573803623169566, 0.9095434294060122, 0.9640898121479401, 0.8857069637290625, 0.9827431981755756, 0.9393819885645368, 0.9421134476050426, 0.9914475819744966, 0.9365859357649092, 0.3604663928680497, 0.630816187519087, 0.44737769601667193, 0.36603629674091337, 0.09489829915505162, 0.013556899879293089, 0.06778449939646544, 0.8729821750586029, 0.9708159136922185, 0.9073041700427378, 0.9632445102714687, 0.9441243728795844, 0.9264749157837281, 0.9715918450854634, 0.815149533990168, 0.915133747942739, 0.29636255239377873, 0.27783989286916755, 0.4260211690660569, 0.9213512807164931, 0.9531598537684085, 0.9624244270703398, 0.3678588308283128, 0.5885741293253005, 0.7022865204831823, 0.2758982759041073, 0.9974090150535575, 0.5596905117320866, 0.2938375186593455, 0.13992262793302165, 0.9798618253745363, 0.8348075181849328, 0.18551278181887396, 0.9469668847050776, 0.9466270484042943, 0.04982247623180497, 0.987133228856473, 0.9169617331753852, 0.9344313312166328, 0.4334647171921725, 0.57795295625623, 0.9726823908611169, 0.8407118529577987, 0.9265143030361691, 0.8610885653472765, 0.9620890554003172, 0.9672128968685691, 0.935761263869363, 0.9297057995931162, 0.9807764361074793, 0.956219450041787, 0.5438029189670659, 0.4531690991392216, 0.9258164772370449, 0.9663159577477226, 0.9822842384565302, 0.9365859357649092, 0.2745326239137432, 0.5795688727067912, 0.12201449951721921, 0.8426258536904473, 0.9700817201403898, 0.9789843808499731, 0.8788367663326955, 0.9895367610676216, 0.9617345586158672, 0.01849489535799745, 0.9774154942248119, 0.39062053909954647, 0.5642296675882338, 0.910480055590429, 0.9661818633621068, 0.880440744546704, 0.9149303326552826, 0.9126756595448979, 0.9297576218887855, 0.9073357488711022, 0.9894520127000084, 0.9588678853539443, 0.9264749157837281, 0.9365859357649092, 0.6330836001662818, 0.07913545002078523, 0.2967579375779446, 0.7630278203349244, 0.22441994715733074, 0.8973834029960734, 0.03041977637274825, 0.06844449683868356, 0.9629587429821342, 0.9355835353599249, 0.894585634564454, 0.9252808242135016, 0.7805929455089209, 0.5171402078194174, 0.2585701039097087, 0.20685608312776696, 0.9963828126190014, 0.8710664655847908, 0.8582920014953129, 0.9162815936417895, 0.9533264408438084, 0.38813371050224554, 0.6037635496701598, 0.870582918228336, 0.08291265887888914, 0.9774057953479673, 0.6542294594993279, 0.297377027045149, 0.09253652058390005, 0.8328286852551005, 0.9404503929060302, 0.8901386583893206, 0.6406637816847998, 0.3355857904063237, 0.9469496764107872, 0.9264749157837281, 0.44953424598239283, 0.28304008080372883, 0.24974124776799603, 0.9268794797668221, 0.984388281503033, 0.8909734846454186, 0.8763130465387559, 0.9231430986931968, 0.929530509532405, 0.913854035953004, 0.9243542900759782, 0.9137280889766055, 0.9804234611748637, 0.9894389714122619, 0.9919755607120322, 0.9771370210385063, 0.9574640836522424], \"Term\": [\"alliance\", \"amazing\", \"amazing\", \"anywhere\", \"area\", \"atmosphere\", \"atmosphere\", \"attentive\", \"back\", \"back\", \"birthday\", \"bit\", \"branch\", \"branch\", \"brasserie\", \"bread\", \"busy\", \"cave\", \"celebration\", \"city\", \"class\", \"classic\", \"close\", \"copy\", \"course\", \"creek\", \"day\", \"delicious\", \"dine\", \"dining\", \"dining\", \"dinner\", \"dinner\", \"dinner\", \"dinner\", \"dinner\", \"disappointed\", \"dish\", \"downside\", \"enough\", \"escargot\", \"etouffee\", \"event\", \"ever\", \"excited\", \"experience\", \"experience\", \"experience\", \"extremely\", \"family\", \"fantastic\", \"favorite\", \"favorite\", \"first\", \"first\", \"food\", \"french\", \"french\", \"french\", \"fresh\", \"fun\", \"fun\", \"glass\", \"good\", \"good\", \"great\", \"hard\", \"high\", \"highly\", \"highly\", \"incredible\", \"interesting\", \"journey\", \"kid\", \"kiss\", \"last\", \"life\", \"lighting\", \"little\", \"location\", \"lovely\", \"lovely\", \"many\", \"martin\", \"meal\", \"melon\", \"menu\", \"menu\", \"menu\", \"moment\", \"mood\", \"mummy\", \"new\", \"nice\", \"night\", \"night\", \"olive\", \"onion\", \"onion\", \"option\", \"overall\", \"overdue\", \"parking\", \"party\", \"pass\", \"pithivier\", \"place\", \"potato\", \"profiterole\", \"prosciutto\", \"really\", \"really\", \"really\", \"reservation\", \"reservation\", \"restaurant\", \"restaurant\", \"restaurant\", \"right\", \"rude\", \"salmon\", \"second\", \"selection\", \"server\", \"server\", \"server\", \"service\", \"shelf\", \"sick\", \"small\", \"son\", \"soup\", \"soup\", \"speak\", \"speak\", \"special\", \"star\", \"star\", \"still\", \"still\", \"super\", \"surprised\", \"table\", \"table\", \"thing\", \"third\", \"time\", \"time\", \"time\", \"tonight\", \"town\", \"truly\", \"upside\", \"urinary\", \"waiter\", \"wall\", \"warm\", \"way\", \"week\", \"well\", \"wine\", \"wonderful\", \"year\"]}, \"R\": 10, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [3, 10, 4, 5, 7, 1, 8, 9, 2, 6]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el15613117161316641008217947\", ldavis_el15613117161316641008217947_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el15613117161316641008217947\", ldavis_el15613117161316641008217947_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el15613117161316641008217947\", ldavis_el15613117161316641008217947_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---------- Topic Modelling ---------- #\n",
    "\n",
    "# We convert the tokens into tuples where we'll have the word index (its placement on the map) and its frequency\n",
    "id2word = corpora.Dictionary(df['tokens'])\n",
    "corpus = [id2word.doc2bow(text) for text in df['tokens']]\n",
    "\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                            id2word=id2word,\n",
    "                                            num_topics=10,\n",
    "                                            random_state=100,\n",
    "                                            update_every=1,\n",
    "                                            chunksize=100,\n",
    "                                            passes=10,\n",
    "                                            alpha='auto',\n",
    "                                            per_word_topics=True)\n",
    "\n",
    "pyLDAvis.enable_notebook(local=True)\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word, mds='mmds', R=10)\n",
    "pyLDAvis.display(vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lda_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_topic_distribution\u001b[39m(lda_model, bow):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lda_model\u001b[38;5;241m.\u001b[39mget_document_topics(bow, minimum_probability\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtopic_distribution\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m [get_topic_distribution(lda_model, corpus[i]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(df))]\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_top_topics\u001b[39m(topic_distribution, num_topics\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# Sort the topics by probability and select the top ones\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(topic_distribution, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;241m1\u001b[39m], reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[:num_topics]\n",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_topic_distribution\u001b[39m(lda_model, bow):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lda_model\u001b[38;5;241m.\u001b[39mget_document_topics(bow, minimum_probability\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtopic_distribution\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m [get_topic_distribution(\u001b[43mlda_model\u001b[49m, corpus[i]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(df))]\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_top_topics\u001b[39m(topic_distribution, num_topics\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# Sort the topics by probability and select the top ones\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(topic_distribution, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;241m1\u001b[39m], reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[:num_topics]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lda_model' is not defined"
     ]
    }
   ],
   "source": [
    "def get_topic_distribution(lda_model, bow):\n",
    "    return lda_model.get_document_topics(bow, minimum_probability=0)\n",
    "\n",
    "df['topic_distribution'] = [get_topic_distribution(lda_model, corpus[i]) for i in range(len(df))]\n",
    "\n",
    "def get_top_topics(topic_distribution, num_topics=5):\n",
    "    # Sort the topics by probability and select the top ones\n",
    "    return sorted(topic_distribution, key=lambda x: x[1], reverse=True)[:num_topics]\n",
    "\n",
    "df['top_topics'] = df['topic_distribution'].apply(lambda x: get_top_topics(x, 11 - 1))\n",
    "\n",
    "def label_topics(topic_list, lda_model):\n",
    "    labels = []\n",
    "    for topic_id, _ in topic_list:\n",
    "        # Get the top words in the topic\n",
    "        words = lda_model.show_topic(topic_id, 5)\n",
    "        # Create a label (e.g., by joining the top words)\n",
    "        label = [word for word, prob in words]\n",
    "        labels.append(label)\n",
    "    return labels\n",
    "\n",
    "def topicise(labels, label_dict):\n",
    "    topics = []\n",
    "\n",
    "    for topic_list in labels:\n",
    "        for key, value in label_dict.items():\n",
    "            if set(topic_list) == set(value):\n",
    "                topics.append(key)\n",
    "\n",
    "    return topics\n",
    "\n",
    "label_dict = {\n",
    "    'Quality of Food & Service' : ['service', 'food', 'restaurant', 'good', 'great'],\n",
    "    'French Dining Experience' : ['dinner', 'meal', 'french', 'reservation', 'little'],\n",
    "    'Atmosphere' : ['speak', 'dining', 'menu', 'experience', 'soup'],\n",
    "    'Price' : ['course', 'table', 'thing', 'life', 'party'],\n",
    "    'Special Occasions' : ['birthday', 'time', 'family', 'really', 'warm'],\n",
    "    'Ambience' : ['experience', 'overall', 'kiss', 'attentive', 'fantastic'],\n",
    "    'Dining Experience' : ['experience', 'overall', 'kiss', 'attentive', 'fantastic'],\n",
    "    'Staff' : ['year', 'last', 'time', 'first', 'second'],\n",
    "    'Menu' : ['atmosphere', 'area', 'bit', 'high', 'mummy'],\n",
    "    'Drinks' : ['way', 'incredible', 'class', 'wall', 'mood'] \n",
    "}\n",
    "\n",
    "df['top_topic_labels'] = df['top_topics'].apply(lambda x: label_topics(x, lda_model))\n",
    "df['topics'] = df['top_topic_labels'].apply(lambda x: topicise(x, label_dict))\n",
    "df.drop(columns=['topic_distribution', 'top_topics'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phrases alogrithm\n",
    "\n",
    "min_count = 3\n",
    "threshold = 5\n",
    "\n",
    "phrases = Phrases(df['tokens'], min_count=min_count, threshold=threshold)\n",
    "phraser = Phraser(phrases)\n",
    "\n",
    "df['bigrams'] = [phraser[tokens] for tokens in df['tokens']]\n",
    "df['trigrams'] = [phraser[bigrams] for bigrams in df['bigrams']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_size = 100\n",
    "window = 5\n",
    "min_count = 1\n",
    "workers = 4\n",
    "\n",
    "# Training the model\n",
    "word2vec_model = Word2Vec(sentences=df['bigrams'], vector_size=vector_size, window=window, min_count=min_count, workers=workers)\n",
    "\n",
    "# Save the model\n",
    "word2vec_model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "argument of type 'Word2Vec' is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m     model_res \u001b[38;5;241m=\u001b[39m model_res\u001b[38;5;241m/\u001b[39mctr\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_res\n\u001b[0;32m---> 15\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvectors\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtokens\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvectorize_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mword2vec_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m X \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvectors\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto_list()\n\u001b[1;32m     18\u001b[0m y \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentiment\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto_list()\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/pandas/core/series.py:4760\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4625\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4626\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4627\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4632\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4633\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4634\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4635\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4636\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4751\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4752\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   4753\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4754\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4755\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4756\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4757\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4758\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4759\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m-> 4760\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/pandas/core/apply.py:1207\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1204\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[1;32m   1206\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[0;32m-> 1207\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/pandas/core/apply.py:1287\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1281\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[1;32m   1282\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[1;32m   1283\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[1;32m   1284\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[1;32m   1285\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[1;32m   1286\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1287\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1288\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[1;32m   1289\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1291\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1292\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1293\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1294\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/pandas/core/base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/pandas/core/algorithms.py:1814\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1812\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1813\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1814\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1815\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1816\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1817\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[1;32m   1818\u001b[0m     )\n",
      "File \u001b[0;32mlib.pyx:2917\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/pandas/core/apply.py:1276\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard.<locals>.curried\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1275\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcurried\u001b[39m(x):\n\u001b[0;32m-> 1276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 9\u001b[0m, in \u001b[0;36mvectorize_model\u001b[0;34m(sent, model)\u001b[0m\n\u001b[1;32m      7\u001b[0m ctr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m sent:\n\u001b[0;32m----> 9\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mword\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m:\n\u001b[1;32m     10\u001b[0m         ctr \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     11\u001b[0m         model_res \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m model[word]\n",
      "\u001b[0;31mTypeError\u001b[0m: argument of type 'Word2Vec' is not iterable"
     ]
    }
   ],
   "source": [
    "file_name = \"word2vec.model\"\n",
    "model = gensim.models.keyedvectors.KeyedVectors.load(file_name)\n",
    "\n",
    "def vectorize_model(sent, model):\n",
    "    vector_size = model.vector_size\n",
    "    model_res = np.zeros(vector_size)\n",
    "    ctr = 1\n",
    "    for word in sent:\n",
    "        if word in model:\n",
    "            ctr += 1\n",
    "            model_res += model[word]\n",
    "    model_res = model_res/ctr\n",
    "    return model_res\n",
    "\n",
    "df['vectors'] = df['tokens'].apply(vectorize_model, model=word2vec_model)\n",
    "\n",
    "X = df['vectors'].to_list()\n",
    "y = df['sentiment'].to_list()\n",
    "\n",
    "test_size = 0.2\n",
    "stratify_value = y\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size,stratify=stratify_value, random_state=42)\n",
    "\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train,y_train)\n",
    "\n",
    "predicted = classifier.predict(X_test)\n",
    "\n",
    "print(\"Logistic Regression Accuracy:\",metrics.accuracy_score(y_test, predicted))\n",
    "print(\"Logistic Regression Precision:\",metrics.precision_score(y_test, predicted, average='micro'))  # Change average to 'micro', 'macro', 'weighted', or None\n",
    "print(\"Logistic Regression Recall:\",metrics.recall_score(y_test, predicted, average='micro'))  # Change average to 'micro', 'macro', 'weighted', or None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 41\u001b[0m\n\u001b[1;32m     32\u001b[0m         plt\u001b[38;5;241m.\u001b[39mannotate(labels[i],\n\u001b[1;32m     33\u001b[0m                      xy\u001b[38;5;241m=\u001b[39m(x[i], y[i]),\n\u001b[1;32m     34\u001b[0m                      xytext\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m2\u001b[39m),\n\u001b[1;32m     35\u001b[0m                      textcoords\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moffset points\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     36\u001b[0m                      ha\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mright\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     37\u001b[0m                      va\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbottom\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     38\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m---> 41\u001b[0m \u001b[43mtsne_plot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword2vec_model\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 17\u001b[0m, in \u001b[0;36mtsne_plot\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     15\u001b[0m tokens \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(tokens)\n\u001b[1;32m     16\u001b[0m tsne_model \u001b[38;5;241m=\u001b[39m TSNE(perplexity\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m, n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, init\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpca\u001b[39m\u001b[38;5;124m'\u001b[39m, n_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2500\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m23\u001b[39m)\n\u001b[0;32m---> 17\u001b[0m new_values \u001b[38;5;241m=\u001b[39m \u001b[43mtsne_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m x \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     22\u001b[0m y \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/sklearn/utils/_set_output.py:157\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 157\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    160\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    161\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    162\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    163\u001b[0m         )\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/sklearn/manifold/_t_sne.py:1111\u001b[0m, in \u001b[0;36mTSNE.fit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m   1090\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fit X into an embedded space and return that transformed output.\u001b[39;00m\n\u001b[1;32m   1091\u001b[0m \n\u001b[1;32m   1092\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;124;03m    Embedding of the training data in low-dimensional space.\u001b[39;00m\n\u001b[1;32m   1109\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1110\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params_vs_input(X)\n\u001b[0;32m-> 1111\u001b[0m embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1112\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_ \u001b[38;5;241m=\u001b[39m embedding\n\u001b[1;32m   1113\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/sklearn/manifold/_t_sne.py:1001\u001b[0m, in \u001b[0;36mTSNE._fit\u001b[0;34m(self, X, skip_num_points)\u001b[0m\n\u001b[1;32m    995\u001b[0m \u001b[38;5;66;03m# Degrees of freedom of the Student's t-distribution. The suggestion\u001b[39;00m\n\u001b[1;32m    996\u001b[0m \u001b[38;5;66;03m# degrees_of_freedom = n_components - 1 comes from\u001b[39;00m\n\u001b[1;32m    997\u001b[0m \u001b[38;5;66;03m# \"Learning a Parametric Embedding by Preserving Local Structure\"\u001b[39;00m\n\u001b[1;32m    998\u001b[0m \u001b[38;5;66;03m# Laurens van der Maaten, 2009.\u001b[39;00m\n\u001b[1;32m    999\u001b[0m degrees_of_freedom \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_components \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m-> 1001\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tsne\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1002\u001b[0m \u001b[43m    \u001b[49m\u001b[43mP\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1003\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdegrees_of_freedom\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1004\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1005\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_embedded\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_embedded\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1006\u001b[0m \u001b[43m    \u001b[49m\u001b[43mneighbors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneighbors_nn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1007\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_num_points\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_num_points\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1008\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/sklearn/manifold/_t_sne.py:1069\u001b[0m, in \u001b[0;36mTSNE._tsne\u001b[0;34m(self, P, degrees_of_freedom, n_samples, X_embedded, neighbors, skip_num_points)\u001b[0m\n\u001b[1;32m   1067\u001b[0m     opt_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmomentum\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.8\u001b[39m\n\u001b[1;32m   1068\u001b[0m     opt_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_iter_without_progress\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter_without_progress\n\u001b[0;32m-> 1069\u001b[0m     params, kl_divergence, it \u001b[38;5;241m=\u001b[39m \u001b[43m_gradient_descent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mopt_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1071\u001b[0m \u001b[38;5;66;03m# Save the final number of iterations\u001b[39;00m\n\u001b[1;32m   1072\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter_ \u001b[38;5;241m=\u001b[39m it\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/sklearn/manifold/_t_sne.py:402\u001b[0m, in \u001b[0;36m_gradient_descent\u001b[0;34m(objective, p0, it, n_iter, n_iter_check, n_iter_without_progress, momentum, learning_rate, min_gain, min_grad_norm, verbose, args, kwargs)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;66;03m# only compute the error when needed\u001b[39;00m\n\u001b[1;32m    400\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompute_error\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m check_convergence \u001b[38;5;129;01mor\u001b[39;00m i \u001b[38;5;241m==\u001b[39m n_iter \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 402\u001b[0m error, grad \u001b[38;5;241m=\u001b[39m \u001b[43mobjective\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    404\u001b[0m inc \u001b[38;5;241m=\u001b[39m update \u001b[38;5;241m*\u001b[39m grad \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m    405\u001b[0m dec \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39minvert(inc)\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/sklearn/manifold/_t_sne.py:283\u001b[0m, in \u001b[0;36m_kl_divergence_bh\u001b[0;34m(params, P, degrees_of_freedom, n_samples, n_components, angle, skip_num_points, verbose, compute_error, num_threads)\u001b[0m\n\u001b[1;32m    280\u001b[0m indptr \u001b[38;5;241m=\u001b[39m P\u001b[38;5;241m.\u001b[39mindptr\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mint64, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    282\u001b[0m grad \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(X_embedded\u001b[38;5;241m.\u001b[39mshape, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m--> 283\u001b[0m error \u001b[38;5;241m=\u001b[39m \u001b[43m_barnes_hut_tsne\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_P\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_embedded\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mneighbors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindptr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m    \u001b[49m\u001b[43mangle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_components\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdof\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdegrees_of_freedom\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    296\u001b[0m c \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2.0\u001b[39m \u001b[38;5;241m*\u001b[39m (degrees_of_freedom \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1.0\u001b[39m) \u001b[38;5;241m/\u001b[39m degrees_of_freedom\n\u001b[1;32m    297\u001b[0m grad \u001b[38;5;241m=\u001b[39m grad\u001b[38;5;241m.\u001b[39mravel()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def tsne_plot(model):\n",
    "    vocab = []\n",
    "    for i in range(0,len(model.wv)):\n",
    "        vocab.append(model.wv.index_to_key[i])\n",
    "\n",
    "    \"Creates and TSNE model and plots it\"\n",
    "    labels = []\n",
    "    tokens = []\n",
    "\n",
    "    for word in vocab:\n",
    "        tokens.append(model.wv[word])\n",
    "        labels.append(word)\n",
    "        #print(tokens)\n",
    "        #print(labels)\n",
    "    tokens = np.array(tokens)\n",
    "    tsne_model = TSNE(perplexity=200, n_components=2, init='pca', n_iter=2500, random_state=23)\n",
    "    new_values = tsne_model.fit_transform(tokens)\n",
    "\n",
    "\n",
    "    \n",
    "    x = []\n",
    "    y = []\n",
    "    for value in new_values:\n",
    "        x.append(value[0])\n",
    "        y.append(value[1])\n",
    "    \n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(16, 16)) \n",
    "    for i in range(len(x)):\n",
    "        plt.scatter(x[i],y[i])\n",
    "        plt.annotate(labels[i],\n",
    "                     xy=(x[i], y[i]),\n",
    "                     xytext=(5, 2),\n",
    "                     textcoords='offset points',\n",
    "                     ha='right',\n",
    "                     va='bottom')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "tsne_plot(word2vec_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-20 14:44:11.424838: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1 Pro\n",
      "2024-01-20 14:44:11.424875: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2024-01-20 14:44:11.424886: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2024-01-20 14:44:11.427800: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:303] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-01-20 14:44:11.430655: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:269] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2024-01-20 14:44:11.463036: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:375] MLIR V1 optimization pass is not enabled\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'projections/model.ckpt-1457'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_name = \"word2vec.model\"\n",
    "model = gensim.models.keyedvectors.KeyedVectors.load(file_name)\n",
    "\n",
    "max_size = len(model.wv.index_to_key)-1\n",
    "\n",
    "w2v = np.zeros((max_size,model.vector_size))\n",
    "\n",
    "if not os.path.exists('projections'):\n",
    "    os.makedirs('projections')\n",
    "    \n",
    "with open(\"projections/metadata.tsv\", 'w+') as file_metadata:\n",
    "    \n",
    "    for i, word in enumerate(model.wv.index_to_key[:max_size]):\n",
    "        \n",
    "        #store the embeddings of the word\n",
    "        w2v[i] = model.wv[word]\n",
    "        \n",
    "        #write the word to a file \n",
    "        file_metadata.write(word + '\\n')\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "sess = tf.compat.v1.InteractiveSession()\n",
    "\n",
    "with tf.device(\"/cpu:0\"):\n",
    "    embedding = tf.Variable(w2v, trainable=False, name='embedding')\n",
    "\n",
    "sess.run(tf.compat.v1.global_variables_initializer())\n",
    "\n",
    "saver = tf.compat.v1.train.Saver()\n",
    "writer = tf.compat.v1.summary.FileWriter('projections', sess.graph)\n",
    "config = projector.ProjectorConfig()\n",
    "embed= config.embeddings.add()\n",
    "\n",
    "embed.tensor_name = 'embedding'\n",
    "embed.metadata_path = 'metadata.tsv'\n",
    "\n",
    "projector.visualize_embeddings(writer, config)\n",
    "saver.save(sess, 'projections/model.ckpt', global_step=max_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wine',\n",
       " 'food',\n",
       " 'dinner',\n",
       " 'french',\n",
       " 'truly',\n",
       " 'great',\n",
       " 'service',\n",
       " 'place',\n",
       " 'old',\n",
       " 'good']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def semantic_search(query_word, model, topn=10):\n",
    "    query_vector = model.wv[query_word]\n",
    "    all_words = model.wv.index_to_key\n",
    "\n",
    "    # Calculate cosine distance between query and all other words\n",
    "    distances = {word: cosine(query_vector, model.wv[word]) for word in all_words}\n",
    "    \n",
    "    # Sort words by distance (lower is more similar)\n",
    "    sorted_words = sorted(distances, key=distances.get)\n",
    "\n",
    "    # Return the topn closest words\n",
    "    return sorted_words[:topn]\n",
    "\n",
    "# Example usage\n",
    "search_results = semantic_search('wine', word2vec_model)\n",
    "\n",
    "search_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've decided to go with this: BERT for Text Classification: Given its context-aware nature and superior performance in understanding nuances, BERT is generally the better choice for text classification.\n",
    "Word2Vec for Semantic Search: For semantic search tasks, Word2Vec's efficiency and effectiveness in finding similar words make it a strong choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokeniser = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
    "\n",
    "def sentiment_score(review):\n",
    "    tokens = tokeniser.encode(review, return_tensors=\"pt\")\n",
    "    result = model(tokens)\n",
    "    return int(torch.argmax(result.logits))+1\n",
    "\n",
    "# Example usage\n",
    "example_sentence = 'It was great, I will come back again.'\n",
    "sentiment_probabilities = sentiment_score(example_sentence)\n",
    "\n",
    "# Apply to DataFrame\n",
    "df['sentiment'] = df['text'].apply(lambda x: sentiment_score(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>rating</th>\n",
       "      <th>location</th>\n",
       "      <th>tokens</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>top_topic_labels</th>\n",
       "      <th>topics</th>\n",
       "      <th>restaurant_id</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Robyn gave amazing service! So attentive and f...</td>\n",
       "      <td>5</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>[amazing, service, attentive, friendly, stuff,...</td>\n",
       "      <td>robin gave amazing service! so attentive and f...</td>\n",
       "      <td>[['experience', 'overall', 'kiss', 'attentive'...</td>\n",
       "      <td>['Ambience', 'Dining Experience', 'Quality of ...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Headed downtown on a Thursday evening for a Ki...</td>\n",
       "      <td>5</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>[downtown, evening, king, game, time, dinner, ...</td>\n",
       "      <td>headed downtown on a thursday evening for a ki...</td>\n",
       "      <td>[['course', 'table', 'thing', 'life', 'party']...</td>\n",
       "      <td>['Price', 'Quality of Food &amp; Service', 'Staff'...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Been here a few times, in just recent weeks. T...</td>\n",
       "      <td>4</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>[time, recent, week, visit, rooftop, bar, time...</td>\n",
       "      <td>been here a few times, in just recent weeks. t...</td>\n",
       "      <td>[['birthday', 'time', 'family', 'really', 'war...</td>\n",
       "      <td>['Special Occasions', 'Quality of Food &amp; Servi...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Service is fast. Staff is friendly. The food i...</td>\n",
       "      <td>5</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>[service, fast, staff, friendly, food, whole, ...</td>\n",
       "      <td>service is fast. staff is friendly. the food i...</td>\n",
       "      <td>[['service', 'food', 'restaurant', 'good', 'gr...</td>\n",
       "      <td>['Quality of Food &amp; Service', 'Special Occasio...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Walked by and asked to see a menu. Very helpfu...</td>\n",
       "      <td>3</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>[menu, helpful, staff, french, concept, try, a...</td>\n",
       "      <td>walked by and asked to see a menu. very helpfu...</td>\n",
       "      <td>[['service', 'food', 'restaurant', 'good', 'gr...</td>\n",
       "      <td>['Quality of Food &amp; Service', 'Atmosphere', 'F...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>613</th>\n",
       "      <td>The Steak Tartare is absolutely yummy! Just as...</td>\n",
       "      <td>5</td>\n",
       "      <td>Phoenix</td>\n",
       "      <td>[tartar, absolutely, mummy, waiter, drop, bran...</td>\n",
       "      <td>the speak tartar is absolutely mummy! just ask...</td>\n",
       "      <td>[['service', 'food', 'restaurant', 'good', 'gr...</td>\n",
       "      <td>['Quality of Food &amp; Service', 'French Dining E...</td>\n",
       "      <td>62</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>614</th>\n",
       "      <td>The culinary journey begins right at your tabl...</td>\n",
       "      <td>5</td>\n",
       "      <td>Miami</td>\n",
       "      <td>[urinary, journey, right, table, fresh, potato...</td>\n",
       "      <td>the urinary journey begins right at your table...</td>\n",
       "      <td>[['course', 'table', 'thing', 'life', 'party']...</td>\n",
       "      <td>['Price', 'Quality of Food &amp; Service', 'Atmosp...</td>\n",
       "      <td>62</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>615</th>\n",
       "      <td>Very nice ambiance. We went there at night, an...</td>\n",
       "      <td>4</td>\n",
       "      <td>New York City</td>\n",
       "      <td>[nice, alliance, night, inside, warm, lighting...</td>\n",
       "      <td>very nice alliance. we went there at night, an...</td>\n",
       "      <td>[['service', 'food', 'restaurant', 'good', 'gr...</td>\n",
       "      <td>['Quality of Food &amp; Service', 'Drinks', 'Speci...</td>\n",
       "      <td>62</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>616</th>\n",
       "      <td>M. whatever ... this is a hard pass.... I know...</td>\n",
       "      <td>1</td>\n",
       "      <td>New York City</td>\n",
       "      <td>[hard, pass, short, rude, thing, bartender, mo...</td>\n",
       "      <td>m. whatever ... this is a hard pass.... i know...</td>\n",
       "      <td>[['service', 'food', 'restaurant', 'good', 'gr...</td>\n",
       "      <td>['Quality of Food &amp; Service', 'Drinks', 'Frenc...</td>\n",
       "      <td>62</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617</th>\n",
       "      <td>Food and service are over the moo wonderful, s...</td>\n",
       "      <td>5</td>\n",
       "      <td>New Orleans</td>\n",
       "      <td>[food, service, wonderful, service, fantastic,...</td>\n",
       "      <td>food and service are over the too wonderful, s...</td>\n",
       "      <td>[['service', 'food', 'restaurant', 'good', 'gr...</td>\n",
       "      <td>['Quality of Food &amp; Service', 'Ambience', 'Din...</td>\n",
       "      <td>62</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>618 rows  9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  rating       location  \\\n",
       "0    Robyn gave amazing service! So attentive and f...       5    Los Angeles   \n",
       "1    Headed downtown on a Thursday evening for a Ki...       5    Los Angeles   \n",
       "2    Been here a few times, in just recent weeks. T...       4    Los Angeles   \n",
       "3    Service is fast. Staff is friendly. The food i...       5    Los Angeles   \n",
       "4    Walked by and asked to see a menu. Very helpfu...       3    Los Angeles   \n",
       "..                                                 ...     ...            ...   \n",
       "613  The Steak Tartare is absolutely yummy! Just as...       5        Phoenix   \n",
       "614  The culinary journey begins right at your tabl...       5          Miami   \n",
       "615  Very nice ambiance. We went there at night, an...       4  New York City   \n",
       "616  M. whatever ... this is a hard pass.... I know...       1  New York City   \n",
       "617  Food and service are over the moo wonderful, s...       5    New Orleans   \n",
       "\n",
       "                                                tokens  \\\n",
       "0    [amazing, service, attentive, friendly, stuff,...   \n",
       "1    [downtown, evening, king, game, time, dinner, ...   \n",
       "2    [time, recent, week, visit, rooftop, bar, time...   \n",
       "3    [service, fast, staff, friendly, food, whole, ...   \n",
       "4    [menu, helpful, staff, french, concept, try, a...   \n",
       "..                                                 ...   \n",
       "613  [tartar, absolutely, mummy, waiter, drop, bran...   \n",
       "614  [urinary, journey, right, table, fresh, potato...   \n",
       "615  [nice, alliance, night, inside, warm, lighting...   \n",
       "616  [hard, pass, short, rude, thing, bartender, mo...   \n",
       "617  [food, service, wonderful, service, fantastic,...   \n",
       "\n",
       "                                          cleaned_text  \\\n",
       "0    robin gave amazing service! so attentive and f...   \n",
       "1    headed downtown on a thursday evening for a ki...   \n",
       "2    been here a few times, in just recent weeks. t...   \n",
       "3    service is fast. staff is friendly. the food i...   \n",
       "4    walked by and asked to see a menu. very helpfu...   \n",
       "..                                                 ...   \n",
       "613  the speak tartar is absolutely mummy! just ask...   \n",
       "614  the urinary journey begins right at your table...   \n",
       "615  very nice alliance. we went there at night, an...   \n",
       "616  m. whatever ... this is a hard pass.... i know...   \n",
       "617  food and service are over the too wonderful, s...   \n",
       "\n",
       "                                      top_topic_labels  \\\n",
       "0    [['experience', 'overall', 'kiss', 'attentive'...   \n",
       "1    [['course', 'table', 'thing', 'life', 'party']...   \n",
       "2    [['birthday', 'time', 'family', 'really', 'war...   \n",
       "3    [['service', 'food', 'restaurant', 'good', 'gr...   \n",
       "4    [['service', 'food', 'restaurant', 'good', 'gr...   \n",
       "..                                                 ...   \n",
       "613  [['service', 'food', 'restaurant', 'good', 'gr...   \n",
       "614  [['course', 'table', 'thing', 'life', 'party']...   \n",
       "615  [['service', 'food', 'restaurant', 'good', 'gr...   \n",
       "616  [['service', 'food', 'restaurant', 'good', 'gr...   \n",
       "617  [['service', 'food', 'restaurant', 'good', 'gr...   \n",
       "\n",
       "                                                topics  restaurant_id  \\\n",
       "0    ['Ambience', 'Dining Experience', 'Quality of ...              1   \n",
       "1    ['Price', 'Quality of Food & Service', 'Staff'...              1   \n",
       "2    ['Special Occasions', 'Quality of Food & Servi...              1   \n",
       "3    ['Quality of Food & Service', 'Special Occasio...              1   \n",
       "4    ['Quality of Food & Service', 'Atmosphere', 'F...              1   \n",
       "..                                                 ...            ...   \n",
       "613  ['Quality of Food & Service', 'French Dining E...             62   \n",
       "614  ['Price', 'Quality of Food & Service', 'Atmosp...             62   \n",
       "615  ['Quality of Food & Service', 'Drinks', 'Speci...             62   \n",
       "616  ['Quality of Food & Service', 'Drinks', 'Frenc...             62   \n",
       "617  ['Quality of Food & Service', 'Ambience', 'Din...             62   \n",
       "\n",
       "     sentiment  \n",
       "0            3  \n",
       "1            2  \n",
       "2            2  \n",
       "3            3  \n",
       "4            3  \n",
       "..         ...  \n",
       "613          3  \n",
       "614          3  \n",
       "615          3  \n",
       "616          1  \n",
       "617          3  \n",
       "\n",
       "[618 rows x 9 columns]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "def push_new_reviews(df):\n",
    "    # Record original data types\n",
    "    original_dtypes = df.dtypes\n",
    "\n",
    "    sentiment_1_rows = df[df['sentiment'] == df['sentiment'].value_counts().index[1]]\n",
    "    sentiment_2_rows = df[df['sentiment'] == df['sentiment'].value_counts().index[2]]\n",
    "    \n",
    "    for i in range(df['sentiment'].value_counts().iloc[0] - df['sentiment'].value_counts().iloc[1] - 1):\n",
    "        random_row = random.choice(sentiment_1_rows.index)\n",
    "        target_row = df.loc[random_row].copy()  # Make a copy of the selected row\n",
    "        \n",
    "        new_review = generate_new_reviews(target_row['cleaned_text'])\n",
    "        target_row['cleaned_text'] = preprocessing(new_review[0])\n",
    "\n",
    "        # Convert target_row to DataFrame and transpose\n",
    "        new_row_df = pd.DataFrame(target_row).transpose()\n",
    "\n",
    "        # Set data types explicitly\n",
    "        for col in new_row_df.columns:\n",
    "            new_row_df[col] = new_row_df[col].astype(original_dtypes[col])\n",
    "\n",
    "        df = pd.concat([df, new_row_df], ignore_index=True)\n",
    "\n",
    "    for i in range(df['sentiment'].value_counts().iloc[0] - df['sentiment'].value_counts().iloc[2] - 1):\n",
    "        random_row = random.choice(sentiment_2_rows.index)\n",
    "        target_row = df.loc[random_row].copy()  # Make a copy of the selected row\n",
    "        \n",
    "        new_review = generate_new_reviews(target_row['cleaned_text'])\n",
    "        target_row['cleaned_text'] = preprocessing(new_review[0])\n",
    "\n",
    "        # Convert target_row to DataFrame and transpose\n",
    "        new_row_df = pd.DataFrame(target_row).transpose()\n",
    "\n",
    "        # Set data types explicitly\n",
    "        for col in new_row_df.columns:\n",
    "            new_row_df[col] = new_row_df[col].astype(original_dtypes[col])\n",
    "\n",
    "        df = pd.concat([df, new_row_df], ignore_index=True)\n",
    "        \n",
    "    return df\n",
    "\n",
    "generated_df = push_new_reviews(df)\n",
    "generated_df = generated_df.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Should we add a tensorboard?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.95      0.97      0.96       102\n",
      "           2       0.91      0.96      0.93       102\n",
      "           3       0.94      0.85      0.89        85\n",
      "\n",
      "    accuracy                           0.93       289\n",
      "   macro avg       0.93      0.93      0.93       289\n",
      "weighted avg       0.93      0.93      0.93       289\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([3])"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(generated_df['cleaned_text'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, generated_df['sentiment'], test_size=0.2, random_state=42)\n",
    "\n",
    "classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "def classify_review(review_text):\n",
    "    # Preprocess the review_text similar to your training data preprocessing\n",
    "    preprocessed_text = preprocessing(review_text)  # Implement this function based on your preprocessing steps\n",
    "    \n",
    "    # Transform the review text to TF-IDF features\n",
    "    tfidf_features = tfidf_vectorizer.transform([preprocessed_text])\n",
    "    \n",
    "    # Predict the label\n",
    "    predicted_label = classifier.predict(tfidf_features)\n",
    "    \n",
    "    return predicted_label\n",
    "\n",
    "classify_review(\"This is a great restaurant! I loved the food and the service was amazing.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-20 15:08:02.257800: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:303] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-01-20 15:08:02.257862: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:269] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "/Users/alexandrecogordan/miniconda3/envs/tensorflow/lib/python3.10/site-packages/tensorflow/python/client/session.py:1770: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'projections/tfidf.ckpt'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorboard.plugins import projector\n",
    "\n",
    "# Assuming X_tfidf is your TF-IDF matrix\n",
    "svd = TruncatedSVD(n_components=50, random_state=42)  # Reduce to 50 dimensions\n",
    "X_reduced = svd.fit_transform(X_tfidf)\n",
    "\n",
    "# Create a projection directory\n",
    "if not os.path.exists('projections'):\n",
    "    os.makedirs('projections')\n",
    "\n",
    "# Save the reduced embeddings and feature names\n",
    "with open(\"projections/metadata.tsv\", 'w+') as file_metadata:\n",
    "    # Use get_feature_names_out() for scikit-learn 0.24 and newer\n",
    "    for feature_name in tfidf_vectorizer.get_feature_names_out():\n",
    "        file_metadata.write(feature_name + '\\n')\n",
    "\n",
    "# TF-IDF embeddings after dimensionality reduction\n",
    "tfidf_embeddings = np.array(X_reduced)\n",
    "\n",
    "# Disable eager execution (needed for TensorBoard in TF1.x)\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "# Start an interactive session\n",
    "sess = tf.compat.v1.InteractiveSession()\n",
    "\n",
    "# Create a TensorFlow variable for the embeddings\n",
    "with tf.device(\"/cpu:0\"):\n",
    "    embedding_var = tf.Variable(tfidf_embeddings, trainable=False, name='tfidf_embedding')\n",
    "\n",
    "# Initialize the variable\n",
    "sess.run(tf.compat.v1.global_variables_initializer())\n",
    "\n",
    "# Save the embeddings in a checkpoint\n",
    "saver = tf.compat.v1.train.Saver([embedding_var])  # Add embedding_var to the saver\n",
    "writer = tf.compat.v1.summary.FileWriter('projections', sess.graph)\n",
    "\n",
    "# Setup the projector configuration\n",
    "config = projector.ProjectorConfig()\n",
    "embedding = config.embeddings.add()\n",
    "embedding.tensor_name = embedding_var.name\n",
    "embedding.metadata_path = 'metadata.tsv'\n",
    "\n",
    "# Visualize embeddings\n",
    "projector.visualize_embeddings(writer, config)\n",
    "saver.save(sess, 'projections/tfidf.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-17 16:33:50.937 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run /Users/alexandrecogordan/miniconda3/envs/tensorflow/lib/python3.10/site-packages/ipykernel_launcher.py [ARGUMENTS]\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "\n",
    "def resize_image(image_path, width, height):\n",
    "    image = Image.open(image_path)\n",
    "    resized_image = image.resize((width, height))\n",
    "    return resized_image\n",
    "\n",
    "st.set_page_config(page_title=\"Gastonomy\", page_icon=\"\", layout=\"wide\")\n",
    "    \n",
    "st.sidebar.markdown(\"Select a city and a restaurant to generate a review.\")\n",
    "city = st.sidebar.selectbox(\"City\", sorted(df['location'].unique()))\n",
    "\n",
    "# Dictionary mapping city names to image filenames\n",
    "city_images = {\n",
    "    'New Orleans': 'resources/new-orleans.jpg',\n",
    "    'New York City': 'resources/new-york.jpg',\n",
    "    'Chicago': 'resources/chicago.jpg',\n",
    "    'Los Angeles': 'resources/los-angeles.jpg',\n",
    "    'San Francisco': 'resources/san-francisco.jpg',\n",
    "    'Philadelphia': 'resources/philadelphia.jpg',\n",
    "    'Las Vegas': 'resources/las-vegas.jpg',\n",
    "    'Houston': 'resources/houston.jpg',\n",
    "    'Phoenix': 'resources/phoenix.jpg',\n",
    "    'Miami': 'resources/miami.jpg'\n",
    "}\n",
    "\n",
    "# Display image based on selected city\n",
    "if city in city_images:\n",
    "    image_filename = city_images[city]\n",
    "    resized_image = resize_image(image_filename, 1920, 1080)\n",
    "    st.image(resized_image, caption=city)\n",
    "else:\n",
    "    st.write(\"Image not found for selected city.\")\n",
    "\n",
    "st.title(\"Restaurant Review Analysis\")\n",
    "\n",
    "st.header(\"Quel sont les aspects les plus importants pour vous dans un restaurant?\")\n",
    "topics = st.multiselect(\"Choisissez vos aspects\", sorted(label_dict.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A faire: Faire choisir a l'utilisateur un ou des topics qu'il aimerait aborder. (S'il est interesse par le food quality par exemple on lui recommandera un restaurant avec une tres bonne food quality - peux-t-on recuperer les restaurants qui correspondent aux avis?).\n",
    "\n",
    "Montrer aussi les topics principaux les plus importants pour les resturants francais de chaque ville\n",
    "\n",
    "Later on, maybe use these topics to enhance the importance of them in summarised reviews.\n",
    "\n",
    "What I suggest is this: Topics principaux par villes. On demande a l'utilisateur de choisir ce qu'il prefere a travers un chatbot et ensuite on trouve les restaurants avec les meilleurs topics et de ce resturant on montre aussi ses meilleurs atouts avec les topics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
