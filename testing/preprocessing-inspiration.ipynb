{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\courn\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\courn\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from spellchecker import SpellChecker\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import FrenchStemmer\n",
    "from unidecode import unidecode\n",
    "import json\n",
    "import random\n",
    "import ast\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# get all compagny url\n",
    "page = 1\n",
    "result_len = 0\n",
    "compagnies_url = []\n",
    "while True:\n",
    "    url = f\"https://fr.trustpilot.com/categories/car_dealer?page={page}\"\n",
    "    response = requests.get(url)\n",
    "    web_page = response.text\n",
    "    soup = BeautifulSoup(web_page, \"html.parser\")\n",
    "\n",
    "    resp = soup.select(\"a[data-business-unit-card-link]\")\n",
    "    for res in resp:\n",
    "        url = res[\"href\"].replace(\"/review/\", \"\")\n",
    "        compagnies_url.append(url)\n",
    "    if result_len == 0:\n",
    "        result_len = len(resp)\n",
    "    elif result_len != len(resp):\n",
    "        break\n",
    "    print(page)\n",
    "    page += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compagny_url(compagny_url):\n",
    "    # Initialize lists\n",
    "    review_titles = []\n",
    "    review_ratings = []\n",
    "    review_texts = []\n",
    "    review_locations = []\n",
    "    page_number = []\n",
    "\n",
    "    # Set Trustpilot page numbers to scrape here\n",
    "    from_page = 1\n",
    "    to_page = 500\n",
    "\n",
    "    for i in range(from_page, to_page + 1):\n",
    "        response = requests.get(f\"https://fr.trustpilot.com/review/{compagny_url}?page={i}\")\n",
    "        web_page = response.text\n",
    "        soup = BeautifulSoup(web_page, \"html.parser\")\n",
    "\n",
    "        if soup.find_all(\"article\") == []:\n",
    "            break\n",
    "\n",
    "        for review in soup.find_all(\"article\"):\n",
    "            # Review titles\n",
    "            review_title = review.select_one(\"a[data-review-title-typography]\")\n",
    "            if review_title == None:\n",
    "                review_titles.append(\"\")\n",
    "            else:\n",
    "                review_titles.append(review_title.getText())\n",
    "\n",
    "            # Review text\n",
    "            review_text = review.select_one(\"p[data-service-review-text-typography]\")\n",
    "            if review_text == None:\n",
    "                review_texts.append(\"\")\n",
    "            else:\n",
    "                review_texts.append(review_text.getText())\n",
    "\n",
    "            # Review ratings\n",
    "            review_rating = review.select_one(\"div[data-service-review-rating]\")\n",
    "            review_ratings.append(review_rating[\"data-service-review-rating\"])\n",
    "                \n",
    "            review_location = review.select_one(\"div[data-consumer-country-typography]\")\n",
    "            if review_location == None:\n",
    "                review_locations.append(\"\")\n",
    "            else:\n",
    "                review_locations.append(review_location.getText())\n",
    "            # Trustpilot page number\n",
    "            page_number.append(i)\n",
    "\n",
    "    # Create final dataframe from lists\n",
    "    return pd.DataFrame(list(zip([compagny_url for i in range(len(review_titles))], review_titles, review_ratings, review_texts, review_locations)),\n",
    "                    columns =['compagny', 'review_title', 'review_rating', 'review_text', 'review_location'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/79 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 79/79 [09:13<00:00,  7.01s/it] \n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "for compagny in tqdm(compagnies_url):\n",
    "    result = compagny_url(compagny)\n",
    "    df = pd.concat([df, result])\n",
    "\n",
    "df.to_csv(\"reviews.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    useless_words = pd.read_csv(\"most_frequent_words_mixed.csv\", header=None)[0].tolist()[:100]\n",
    "except:\n",
    "    useless_words = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\courn\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFCamembertForSequenceClassification.\n",
      "\n",
      "All the layers of TFCamembertForSequenceClassification were initialized from the model checkpoint at tblard/tf-allocine.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFCamembertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "STEMMER = FrenchStemmer()\n",
    "spell = SpellChecker(language='fr')\n",
    "pipe = pipeline(\"text-classification\", model=\"tblard/tf-allocine\")\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Suppression des accents\n",
    "    text = unidecode(text)\n",
    "    # Suppression du code HTML\n",
    "    text = re.sub(re.compile(\"<.*?>\"), \"\", text)\n",
    "    text = re.sub(r'[^a-zA-Z0-9/s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    # Suppresssion des nombres\n",
    "    text = re.sub(r'[0-9]+', ' ', text)\n",
    "    # Supprimer les lignes vides\n",
    "    text = text.split('\\n')\n",
    "    text = [line.strip() for line in text if len(line) > 0]\n",
    "    text = ' '.join(text)\n",
    "    # Supprimer les liens\n",
    "    text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "    # Lemmatiser les mots\n",
    "    tokens = word_tokenize(text.lower(), language='french')\n",
    "    return tokens\n",
    "\n",
    "n_grams = lambda tokens, n: [\" \".join(tokens[i:i+n]) for i in range(len(tokens)-n+1)]\n",
    "\n",
    "def text2Token(text, spelling = True, stem = True, len_word_min = 2, spell = spell, useless_words = useless_words):\n",
    "    stopword = stopwords.words('french')\n",
    "    word_tokens = preprocess_text(text)\n",
    "    word_tokens = [word for word in word_tokens if word not in stopword and word not in useless_words and len(word) > len_word_min]\n",
    "    if spelling:\n",
    "        word_tokens = [spell.correction(word) for word in word_tokens]\n",
    "        word_tokens = [word for word in word_tokens if word != None]\n",
    "    if stem:\n",
    "        word_tokens = [STEMMER.stem(token) for token in word_tokens]\n",
    "    word_tokens_with_n_grams = word_tokens + n_grams(word_tokens, 2) + n_grams(word_tokens, 3)\n",
    "    return word_tokens_with_n_grams\n",
    "\n",
    "def getMostFrequentWords(documents, top=10):\n",
    "    # Compter les mots\n",
    "    word_count = {}\n",
    "    for doc in documents:\n",
    "        for word in doc:\n",
    "            if word in word_count:\n",
    "                word_count[word] += 1\n",
    "            else:\n",
    "                word_count[word] = 1\n",
    "\n",
    "    # Trier les mots par fr√©quence d√©croissante\n",
    "    word_count = sorted(word_count.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    if top == float('inf'):\n",
    "        return word_count\n",
    "    \n",
    "    return word_count[:top]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocess data\n"
     ]
    }
   ],
   "source": [
    "# Preprocess data\n",
    "print(\"Preprocess data\")\n",
    "df = pd.read_csv(\"reviews.csv\")\n",
    "df = df.dropna(subset=['review_text'])\n",
    "df = df[df[\"review_location\"]  == \"FR\"]\n",
    "df = df.drop_duplicates(subset=['review_text'])\n",
    "df = df.reset_index(drop=True)\n",
    "df = df[['review_text', 'review_rating']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get most frequent words list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/16933 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 19/16933 [00:09<2:19:33,  2.02it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m df_prepared_data \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(df))):\n\u001b[1;32m----> 3\u001b[0m     df_prepared_data\u001b[38;5;241m.\u001b[39mappend(\u001b[43mtext2Token\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mat\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreview_text\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43museless_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[1;32mIn[3], line 31\u001b[0m, in \u001b[0;36mtext2Token\u001b[1;34m(text, spelling, stem, len_word_min, spell, useless_words)\u001b[0m\n\u001b[0;32m     29\u001b[0m word_tokens \u001b[38;5;241m=\u001b[39m [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m word_tokens \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stopword \u001b[38;5;129;01mand\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m useless_words \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(word) \u001b[38;5;241m>\u001b[39m len_word_min]\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spelling:\n\u001b[1;32m---> 31\u001b[0m     word_tokens \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mspell\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcorrection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mword\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mword_tokens\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     32\u001b[0m     word_tokens \u001b[38;5;241m=\u001b[39m [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m word_tokens \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stem:\n",
      "Cell \u001b[1;32mIn[3], line 31\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     29\u001b[0m word_tokens \u001b[38;5;241m=\u001b[39m [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m word_tokens \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stopword \u001b[38;5;129;01mand\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m useless_words \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(word) \u001b[38;5;241m>\u001b[39m len_word_min]\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spelling:\n\u001b[1;32m---> 31\u001b[0m     word_tokens \u001b[38;5;241m=\u001b[39m [\u001b[43mspell\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcorrection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m word_tokens]\n\u001b[0;32m     32\u001b[0m     word_tokens \u001b[38;5;241m=\u001b[39m [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m word_tokens \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stem:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\spellchecker\\spellchecker.py:159\u001b[0m, in \u001b[0;36mSpellChecker.correction\u001b[1;34m(self, word)\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"The most probable correct spelling for the word\u001b[39;00m\n\u001b[0;32m    153\u001b[0m \n\u001b[0;32m    154\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;124;03m    word (str): The word to correct\u001b[39;00m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;124;03mReturns:\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;124;03m    str: The most likely candidate or None if no correction is present\"\"\"\u001b[39;00m\n\u001b[0;32m    158\u001b[0m word \u001b[38;5;241m=\u001b[39m ensure_unicode(word)\n\u001b[1;32m--> 159\u001b[0m candidates \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcandidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m candidates:\n\u001b[0;32m    161\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\spellchecker\\spellchecker.py:186\u001b[0m, in \u001b[0;36mSpellChecker.candidates\u001b[1;34m(self, word)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;66;03m# if still not found, use the edit distance 1 to calc edit distance 2\u001b[39;00m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_distance \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m--> 186\u001b[0m     tmp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mknown(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__edit_distance_alt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tmp:\n\u001b[0;32m    188\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m tmp\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\spellchecker\\spellchecker.py:253\u001b[0m, in \u001b[0;36mSpellChecker.__edit_distance_alt\u001b[1;34m(self, words)\u001b[0m\n\u001b[0;32m    251\u001b[0m tmp_words \u001b[38;5;241m=\u001b[39m [ensure_unicode(w) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m words]\n\u001b[0;32m    252\u001b[0m tmp \u001b[38;5;241m=\u001b[39m [w \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_case_sensitive \u001b[38;5;28;01melse\u001b[39;00m w\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m tmp_words \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_if_should_check(w)]\n\u001b[1;32m--> 253\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[43me2\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43me1\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtmp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43me2\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mknown\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medit_distance_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43me1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\spellchecker\\spellchecker.py:253\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    251\u001b[0m tmp_words \u001b[38;5;241m=\u001b[39m [ensure_unicode(w) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m words]\n\u001b[0;32m    252\u001b[0m tmp \u001b[38;5;241m=\u001b[39m [w \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_case_sensitive \u001b[38;5;28;01melse\u001b[39;00m w\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m tmp_words \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_if_should_check(w)]\n\u001b[1;32m--> 253\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [e2 \u001b[38;5;28;01mfor\u001b[39;00m e1 \u001b[38;5;129;01min\u001b[39;00m tmp \u001b[38;5;28;01mfor\u001b[39;00m e2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mknown(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medit_distance_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43me1\u001b[49m\u001b[43m)\u001b[49m)]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\spellchecker\\spellchecker.py:228\u001b[0m, in \u001b[0;36mSpellChecker.edit_distance_1\u001b[1;34m(self, word)\u001b[0m\n\u001b[0;32m    226\u001b[0m deletes \u001b[38;5;241m=\u001b[39m [L \u001b[38;5;241m+\u001b[39m R[\u001b[38;5;241m1\u001b[39m:] \u001b[38;5;28;01mfor\u001b[39;00m L, R \u001b[38;5;129;01min\u001b[39;00m splits \u001b[38;5;28;01mif\u001b[39;00m R]\n\u001b[0;32m    227\u001b[0m transposes \u001b[38;5;241m=\u001b[39m [L \u001b[38;5;241m+\u001b[39m R[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m R[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m R[\u001b[38;5;241m2\u001b[39m:] \u001b[38;5;28;01mfor\u001b[39;00m L, R \u001b[38;5;129;01min\u001b[39;00m splits \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(R) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m--> 228\u001b[0m replaces \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mL\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mR\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mR\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msplits\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mR\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mletters\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    229\u001b[0m inserts \u001b[38;5;241m=\u001b[39m [L \u001b[38;5;241m+\u001b[39m c \u001b[38;5;241m+\u001b[39m R \u001b[38;5;28;01mfor\u001b[39;00m L, R \u001b[38;5;129;01min\u001b[39;00m splits \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m letters]\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mset\u001b[39m(deletes \u001b[38;5;241m+\u001b[39m transposes \u001b[38;5;241m+\u001b[39m replaces \u001b[38;5;241m+\u001b[39m inserts)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\spellchecker\\spellchecker.py:228\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    226\u001b[0m deletes \u001b[38;5;241m=\u001b[39m [L \u001b[38;5;241m+\u001b[39m R[\u001b[38;5;241m1\u001b[39m:] \u001b[38;5;28;01mfor\u001b[39;00m L, R \u001b[38;5;129;01min\u001b[39;00m splits \u001b[38;5;28;01mif\u001b[39;00m R]\n\u001b[0;32m    227\u001b[0m transposes \u001b[38;5;241m=\u001b[39m [L \u001b[38;5;241m+\u001b[39m R[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m R[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m R[\u001b[38;5;241m2\u001b[39m:] \u001b[38;5;28;01mfor\u001b[39;00m L, R \u001b[38;5;129;01min\u001b[39;00m splits \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(R) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m--> 228\u001b[0m replaces \u001b[38;5;241m=\u001b[39m [L \u001b[38;5;241m+\u001b[39m c \u001b[38;5;241m+\u001b[39m R[\u001b[38;5;241m1\u001b[39m:] \u001b[38;5;28;01mfor\u001b[39;00m L, R \u001b[38;5;129;01min\u001b[39;00m splits \u001b[38;5;28;01mif\u001b[39;00m R \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m letters]\n\u001b[0;32m    229\u001b[0m inserts \u001b[38;5;241m=\u001b[39m [L \u001b[38;5;241m+\u001b[39m c \u001b[38;5;241m+\u001b[39m R \u001b[38;5;28;01mfor\u001b[39;00m L, R \u001b[38;5;129;01min\u001b[39;00m splits \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m letters]\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mset\u001b[39m(deletes \u001b[38;5;241m+\u001b[39m transposes \u001b[38;5;241m+\u001b[39m replaces \u001b[38;5;241m+\u001b[39m inserts)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df_prepared_data = []\n",
    "for i in tqdm(range(len(df))):\n",
    "    df_prepared_data.append(text2Token(df.at[i, \"review_text\"], stem=False, useless_words=[]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_frequent_words = getMostFrequentWords(df_prepared_data, top=1000)\n",
    "pd.DataFrame(most_frequent_words, columns=[\"word\", \"count\"]).to_csv(\"most_frequent_words.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traslate most frequent words list to english (for better performance of the pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_frequent_words = pd.read_csv(\"most_frequent_words.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"en\" not in most_frequent_words.columns:\n",
    "    most_frequent_words[\"en\"] = [None for i in range(len(most_frequent_words))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import translators as ts\n",
    "\n",
    "def translate(word):\n",
    "    try:\n",
    "        return ts.tencent(word, to_language=\"en\", from_language=\"fr\")\n",
    "    except Exception as e:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [47:31<00:00,  2.85s/it] \n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(len(most_frequent_words))):\n",
    "    if most_frequent_words.at[i, \"en\"] == None:\n",
    "        most_frequent_words.at[i, \"en\"] = translate(most_frequent_words.at[i, \"word\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_frequent_words.to_csv(\"most_frequent_words.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the sentiment of the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_frequent_words = pd.read_csv(\"most_frequent_words.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"sentiment\" in most_frequent_words.columns:\n",
    "    most_frequent_words[\"sentiment\"] = [None for i in range(len(most_frequent_words))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [04:29<00:00,  3.71it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(len(most_frequent_words))):\n",
    "    if most_frequent_words.at[i, \"sentiment\"] == None and str(most_frequent_words.at[i, \"word\"]) != \"nan\":\n",
    "        pipe_res = pipe(most_frequent_words.at[i, \"word\"])[0]\n",
    "        most_frequent_words.at[i, \"sentiment\"] = pipe_res[\"label\"] if pipe_res[\"score\"] > 0.65 else \"MIXED\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_frequent_words.to_csv(\"most_frequent_words.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_frequent_words[most_frequent_words[\"sentiment\"] == \"MIXED\"][[\"word\"]].to_csv(\"most_frequent_words_mixed.csv\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "useless_words = most_frequent_words[most_frequent_words[\"sentiment\"] == \"MIXED\"][\"word\"].tolist()[100:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate train dataset\n",
      "Spell check for train dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1905 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1905/1905 [17:02<00:00,  1.86it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save train dataset\n",
      "Generate test dataset\n",
      "Save test dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate train dataset\n",
    "print(\"Generate train dataset\")\n",
    "train = df.copy()\n",
    "rus = RandomUnderSampler(random_state=0)\n",
    "X = train[[\"review_text\"]]\n",
    "y = train[\"review_rating\"]\n",
    "X_resampled, y_resampled = rus.fit_resample(X, y)\n",
    "train = pd.concat([X_resampled, y_resampled], axis=1)\n",
    "train_idx = train.index.tolist()\n",
    "train = train.reset_index(drop=True)\n",
    "\n",
    "# Spell check for train dataset\n",
    "print(\"Spell check for train dataset\")\n",
    "for i in tqdm(range(len(train))):\n",
    "    train.at[i, \"review_text\"] = json.dumps(text2Token(train.at[i, \"review_text\"]))\n",
    "\n",
    "# Save train dataset\n",
    "print(\"Save train dataset\")\n",
    "train.to_csv(\"reviews_train.csv\", index=False)\n",
    "\n",
    "# Generate test dataset\n",
    "print(\"Generate test dataset\")\n",
    "test = df.copy()\n",
    "test = test.drop(train_idx)\n",
    "rus = RandomUnderSampler(random_state=0)\n",
    "X = test[[\"review_text\"]]\n",
    "y = test[\"review_rating\"]\n",
    "X_resampled, y_resampled = rus.fit_resample(X, y)\n",
    "test = pd.concat([X_resampled, y_resampled], axis=1)\n",
    "\n",
    "# Save test dataset\n",
    "print(\"Save test dataset\")\n",
    "test.to_csv(\"reviews_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"reviews_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"review_text\"] = train[\"review_text\"].apply(lambda x: ast.literal_eval(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "review_rating\n",
       "1    381\n",
       "2    381\n",
       "3    381\n",
       "4    381\n",
       "5    381\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"review_rating\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "# Liste d'avis\n",
    "documents = train[\"review_text\"].tolist()\n",
    "\n",
    "# Listes de scores\n",
    "ratings = train[\"review_rating\"].tolist()\n",
    "\n",
    "# Cr√©er un mod√®le BM25\n",
    "bm25 = BM25Okapi(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTopDocs(bm25, query, documents, ratings, top=5):\n",
    "    # Calculer les scores de similarit√©\n",
    "    scores = bm25.get_scores(query)\n",
    "\n",
    "    # Associer chaque avis √† son score\n",
    "    doc_scores = list(zip(documents, scores, ratings))\n",
    "\n",
    "    # Trier les avis par score d√©croissant\n",
    "    return sorted(doc_scores, key=lambda x: x[1], reverse=True)[:top]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Not used\n",
    "# Obtenir les mots positifs et n√©gatifs en utilisant bert\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "\n",
    "def getSentiment(text):\n",
    "    # Tokenize the text\n",
    "    tokens = tokenizer.encode(text, return_tensors='pt')\n",
    "    # Reduce the tokens size to 512\n",
    "    tokens = tokens[:, :512]\n",
    "    # Get the prediction\n",
    "    result = model(tokens)\n",
    "    # Return the label\n",
    "    return torch.argmax(result.logits).item()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Not used\n",
    "def getRevelantWords(most_freq, pos_nb = 5, neg_nb = 5):\n",
    "    pos_words = []\n",
    "    neg_words = []\n",
    "    for word, nb in most_freq:\n",
    "        if type(word) == tuple:\n",
    "            word = \" \".join(word)\n",
    "        if getSentiment(word) == 0 and len(neg_words) < neg_nb:\n",
    "            neg_words.append(word)\n",
    "        elif getSentiment(word) == 4 and len(pos_words) < pos_nb:\n",
    "            pos_words.append(word)\n",
    "\n",
    "        if len(neg_words) == neg_nb and len(pos_words) == pos_nb:\n",
    "            break\n",
    "\n",
    "    return pos_words, neg_words\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separer_phrase(phrase):\n",
    "    # On ajoute des points aux sauts de ligne\n",
    "    phrase = phrase.replace('\\n', '.')\n",
    "\n",
    "    # Divise d'abord la phrase en utilisant les points, points d'interrogation, points d'exclamation.\n",
    "    pattern = r'(?<=[.!?])(?=\\s|[A-Z\"\\'(])'\n",
    "    groupes = re.split(pattern, phrase)\n",
    "\n",
    "    groupes_fins = []\n",
    "    for groupe in groupes:\n",
    "        # Combinaison des motifs de virgule et \"et\" en une seule expression r√©guli√®re\n",
    "        # S√©pare sur les virgules (en √©vitant les nombres d√©cimaux), sur les ; et : et sur les conjonctions de coordinations (avec un contexte sp√©cifique)\n",
    "        pattern_combined = r'(?<=.{20},)\\s(?!\\d)|[;:]|\\b(mais|ou|et|donc|or|ni|car)\\b(?=.{15,})'\n",
    "        sous_groupes = re.split(pattern_combined, groupe)\n",
    "        groupes_fins.extend(sous_groupes)\n",
    "\n",
    "    return [groupe.strip() for groupe in groupes_fins if groupe is not None and groupe.strip() and groupe.strip() not in ['.', ',', 'mais', 'ou', 'et', 'donc', 'or', 'ni', 'car']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_score(top_docs, origin_query = None, use_bm25 = True, FIABILITY_THRESHOLD = 0.6):\n",
    "    # Calculer la note moyenne des avis\n",
    "    stars_bm25 = None\n",
    "    if use_bm25:\n",
    "        stars_bm25 = sum([int(doc[2]) for doc in top_docs]) / len(top_docs)\n",
    "        if origin_query is None:\n",
    "            return stars_bm25\n",
    "    try:\n",
    "        score_pipe = pipe(origin_query)\n",
    "        stars_pipe = 2.5 + 2.5 * FIABILITY_THRESHOLD if score_pipe[0][\"label\"] == \"POSITIVE\" else (2.5 - 2.5 * FIABILITY_THRESHOLD if score_pipe[0][\"label\"] == \"NEGATIVE\" else 2.5)\n",
    "        return (stars_bm25 + stars_pipe) / 2\n",
    "    except:\n",
    "        return stars_bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRevelantSentences(origin_query, most_freq, documents, ratings, top=5, use_bm25 = True, use_pipe = True):\n",
    "\n",
    "    # Appel de la fonction\n",
    "    groupes = separer_phrase(origin_query)\n",
    "\n",
    "    # Obtenir les scores de chaque groupe\n",
    "    scores = []\n",
    "    for groupe in groupes:\n",
    "        if use_pipe and use_bm25:\n",
    "            scores.append(estimate_score(getTopDocs(bm25, text2Token(groupe), documents, ratings), origin_query))\n",
    "        elif use_pipe:\n",
    "            scores.append(estimate_score(None, origin_query, use_bm25=False))\n",
    "        elif use_bm25:\n",
    "            scores.append(estimate_score(getTopDocs(bm25, text2Token(groupe), documents, ratings)))\n",
    "            \n",
    "    pos_list = []\n",
    "    neg_list = []\n",
    "    for groupe, score in zip(groupes, scores):\n",
    "        group_tokens = text2Token(groupe)\n",
    "        sumFreq = sum([freq for word, freq in most_freq if word in group_tokens])\n",
    "        if score >= 3.5:\n",
    "            pos_list.append((groupe, sumFreq))\n",
    "        elif score <= 1.5:\n",
    "            neg_list.append((groupe, sumFreq))\n",
    "\n",
    "    pos_list = [sentence[0] for sentence in sorted(pos_list, key=lambda x: x[1], reverse=True)[:top]]\n",
    "    neg_list = [sentence[0] for sentence in sorted(neg_list, key=lambda x: x[1], reverse=True)[:top]]\n",
    "\n",
    "    return pos_list, neg_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(origin_query, bm25=bm25, documents=documents, ratings=ratings, spell=spell, use_bm25 = True, use_pipe = True):\n",
    "    query = text2Token(origin_query)\n",
    "    if use_bm25:\n",
    "        top_docs = getTopDocs(bm25, query, documents, ratings, top=5)\n",
    "    most_freq = getMostFrequentWords([doc[0] for doc in top_docs], top=50)\n",
    "    \"\"\"  Not used\n",
    "    relevants = getRevelantWords(most_freq, pos_nb = 5, neg_nb = 5)\n",
    "    \"\"\"\n",
    "\n",
    "    pos_list, neg_list = getRevelantSentences(origin_query, most_freq, documents, ratings, top=5, use_bm25 = True, use_pipe = True)\n",
    "    if not use_pipe:\n",
    "        origin_query = None\n",
    "    return estimate_score(top_docs, origin_query, use_bm25=use_bm25), pos_list, neg_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use data unsed and balanced the new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "review_rating\n",
       "1    152\n",
       "3    152\n",
       "4    152\n",
       "5    152\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv(\"reviews_test.csv\")\n",
    "\n",
    "test[\"review_rating\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score_only(origin_query):\n",
    "    query = text2Token(origin_query)\n",
    "    top_docs = getTopDocs(bm25, query, documents, ratings, top=5)\n",
    "    return estimate_score(top_docs, origin_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/608 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 608/608 [09:06<00:00,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.7905030231971882\n",
      "MSE: 1.128923385959617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = test[\"review_text\"], test[\"review_rating\"]\n",
    "\n",
    "y_pred = [get_score_only(query) for query in tqdm(X_test, total=len(X_test))]\n",
    "print(f\"MAE: {mean_absolute_error(y_test, y_pred)}\")\n",
    "print(f\"MSE: {mean_squared_error(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Voiture achet√©e le 19 ao√ªt, tomb√©e en panne 1 semaine apr√®s (voyant d√©faut moteur rouge). Rest√©e 2 semaines sur place pour la r√©cup√©rer et retomb√©e en panne au bout de 10 km. V√©hicule immobilis√© √† nouveau 1 semaine chez eux. Pour retomber en panne au bout de 500km. Envoy√©e chez le concessionnaire Nissan car plus confiance aux comp√©tences des m√©caniciens. En esp√©rant ne plus retomber en panne. Malgr√© tout, heureusement que le service commercial rattrape un peu les choses...\n",
      "Vrai score: 1\n",
      "Score: 1.7\n",
      "S√©quences positives: []\n",
      "S√©quences n√©gatives: ['retomb√©e en panne au bout de 10 km.', 'Pour retomber en panne au bout de 500km.', 'En esp√©rant ne plus retomber en panne.', 'V√©hicule immobilis√© √† nouveau 1 semaine chez eux.']\n"
     ]
    }
   ],
   "source": [
    "elmt = test.iloc[random.randint(0, len(test))]\n",
    "origin_query = elmt[\"review_text\"]\n",
    "score, pos_list, neg_list = main(origin_query)\n",
    "\n",
    "print(f\"Query: {origin_query}\")\n",
    "print(f\"Vrai score: {elmt['review_rating']}\")\n",
    "\n",
    "print(f\"Score: {score}\")\n",
    "print(f\"S√©quences positives: {pos_list}\")\n",
    "print(f\"S√©quences n√©gatives: {neg_list}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "\n",
    "# Importez ici vos fonctions de pr√©diction (prediction_1, prediction_2, prediction_3)\n",
    "\n",
    "def prediction_1(origin_query):\n",
    "    score, pos_list, neg_list = main(origin_query, use_pipe=False)\n",
    "    return {\n",
    "        \"nombre d √©toile sur 5\": score,\n",
    "        \"liste phrases positives\": pos_list,\n",
    "        \"liste phrases n√©gatives\": neg_list\n",
    "    }\n",
    "\n",
    "def prediction_2(origin_query):\n",
    "    score, pos_list, neg_list = main(origin_query, use_bm25=False)\n",
    "    return {\n",
    "        \"nombre d √©toile sur 5\": score,\n",
    "        \"liste phrases positives\": pos_list,\n",
    "        \"liste phrases n√©gatives\": neg_list\n",
    "    }\n",
    "\n",
    "def prediction_3(origin_query):\n",
    "    score, pos_list, neg_list = main(origin_query)\n",
    "    return {\n",
    "        \"nombre d √©toile sur 5\": score,\n",
    "        \"liste phrases positives\": pos_list,\n",
    "        \"liste phrases n√©gatives\": neg_list\n",
    "    }\n",
    "\n",
    "def afficher_resultats(resultats):\n",
    "    st.subheader(\"R√©sultats de la pr√©diction :\")\n",
    "    st.write(f\"Nombre d'√©toiles sur 5 : {'üåü' * resultats['nombre d √©toile sur 5']}\")\n",
    "    st.subheader(\"Liste de phrases positives :\")\n",
    "    for phrase in resultats[\"liste phrases positives\"]:\n",
    "        st.write(f\"üëç {phrase}\")\n",
    "    st.subheader(\"Liste de phrases n√©gatives :\")\n",
    "    for phrase in resultats[\"liste phrases n√©gatives\"]:\n",
    "        st.write(f\"üëé {phrase}\")\n",
    "\n",
    "def run():\n",
    "    st.title(\"Analyse d'avis Internet\")\n",
    "\n",
    "    avis_utilisateur = st.text_area(\"Entrez votre avis ici :\")\n",
    "\n",
    "    if st.button(\"Pr√©diction BM 25\"):\n",
    "        resultats_1 = prediction_1(avis_utilisateur)\n",
    "        afficher_resultats(resultats_1)\n",
    "\n",
    "    if st.button(\"Pr√©diction Transformers\"):\n",
    "        resultats_2 = prediction_2(avis_utilisateur)\n",
    "        afficher_resultats(resultats_2)\n",
    "\n",
    "    if st.button(\"Pr√©diction BM 25 + Transformers\"):\n",
    "        resultats_3 = prediction_3(avis_utilisateur)\n",
    "        afficher_resultats(resultats_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
