{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Application - Restaurant Review Analysis üë®‚Äçüç≥\n",
    "## By BROSSEAU Alexandre & COGORDAN Alexandre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Our imports\n",
    "\n",
    "import nltk\n",
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "import spacy\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import matplotlib.pyplot as plt\n",
    "import streamlit as st\n",
    "import torch\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gensim.downloader as api\n",
    "import tensorflow_hub as hub\n",
    "import nlpaug.augmenter.word as naw\n",
    "import random\n",
    "import xgboost as xgb\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from textblob import TextBlob\n",
    "from collections import Counter\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification, AutoModel, AutoTokenizer\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel, Word2Vec, KeyedVectors\n",
    "from sklearn.manifold import TSNE\n",
    "from PIL import Image\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tensorboard.plugins import projector\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from tensorboard.plugins import projector\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from scipy.spatial.distance import euclidean, cosine\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from nlpaug.util import Action\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score, roc_curve, roc_auc_score, precision_recall_curve, average_precision_score\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# We set a random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# We load our environment variables - API, HuggingFace, OPENAI keys\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading our dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "com1: We're re-using the csv files that had previously ran analysis so that we avoid running the calculations multiple times and waste time on computational power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('yelp_reviews.csv')\n",
    "\n",
    "# We drop duplicates and null values\n",
    "df.drop_duplicates(inplace=True)\n",
    "df.dropna(subset=['text', 'rating', 'location'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translation pipeline\n",
    "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-zh-en\")\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['text'] = df['text'].astype(str)\n",
    "\n",
    "# Check if text contains Chinese characters\n",
    "def contains_chinese(text):\n",
    "    return bool(re.search('[\\u4e00-\\u9fff]', text))\n",
    "\n",
    "# Translation function (from Chinese to English)\n",
    "def translate_text(text):\n",
    "    if contains_chinese(text):\n",
    "        return translator(text)[0]['translation_text']\n",
    "    else:\n",
    "        return text\n",
    "    \n",
    "# We remove special characters/punctuation whilst keeping characters with accents since their meaning is important in French restaurants reviews\n",
    "def remove_special_characters(text):\n",
    "    pattern = r\"[^\\w\\s√©√®√†√ß√™√¥≈ì]\"\n",
    "    \n",
    "    # We substitute matched characters with an empty string\n",
    "    cleaned_text = re.sub(pattern, '', text)\n",
    "    \n",
    "    return cleaned_text\n",
    "    \n",
    "# Lemmatisation & Tokenisation function\n",
    "def tokenisation(reviews, allowed_postags=[\"NOUN\", \"ADJ\", \"VERBS\", \"ADV\"]):\n",
    "    nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "    reviews_out = []\n",
    "    tokens = []\n",
    "\n",
    "    for review in reviews:\n",
    "        doc = nlp(review) \n",
    "        reviews_out.append(\" \".join([token.lemma_ for token in doc if token.pos_ in allowed_postags and token.lemma_ not in stop_words]))\n",
    "    \n",
    "    for text in reviews_out:\n",
    "        new = gensim.utils.simple_preprocess(text, deacc=False) # We do not remove the accent marks because we deem them important for French restaurants reviews\n",
    "        tokens.append(new)\n",
    "\n",
    "    return tokens\n",
    "\n",
    "# Preprocessing function - which regroups all of the previous steps too\n",
    "def preprocessing(text):\n",
    "    # Translation\n",
    "    translated_text = translate_text(str(text))\n",
    "\n",
    "    # We remove special characters/punctuation\n",
    "    corrected_text = remove_special_characters(translated_text)\n",
    "\n",
    "    # Corrected spelling on lower case text\n",
    "    cleaned_text = str(TextBlob(corrected_text.lower()).correct())\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "# Apply preprocessing and tokenisation\n",
    "df['cleaned_text'] = df['text'].apply(preprocessing)\n",
    "df['tokens'] = tokenisation(df['cleaned_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We review the most frequent words "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "com2: This allows us to get an understanding of the most frequent words found in reviews for each restaurant. This gives a broad introduction to the feeling people get in a specific restaurant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_freq</th>\n",
       "      <th>bigram_freq</th>\n",
       "      <th>trigram_freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'time': 5, 'service': 3, 'food': 3, 'amazing'...</td>\n",
       "      <td>{('top', 'tier'): 2, ('amazing', 'service'): 1...</td>\n",
       "      <td>{('amazing', 'service', 'attentive'): 1, ('ser...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'food': 6, 'service': 4, 'life': 3, 'heart': ...</td>\n",
       "      <td>{('heart', 'belly'): 1, ('belly', 'happily'): ...</td>\n",
       "      <td>{('heart', 'belly', 'happily'): 1, ('belly', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'food': 6, 'service': 5, 'time': 4, 'great': ...</td>\n",
       "      <td>{('first', 'time'): 2, ('food', 'good'): 2, ('...</td>\n",
       "      <td>{('review', 'private', 'party'): 1, ('private'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'restaurant': 3, 'food': 3, 'pm': 3, 'great':...</td>\n",
       "      <td>{('thoughtful', 'son'): 1, ('son', 'generous')...</td>\n",
       "      <td>{('thoughtful', 'son', 'generous'): 1, ('son',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>{'service': 3, 'food': 3, 'experience': 3, 'go...</td>\n",
       "      <td>{('great', 'thing'): 1, ('thing', 'place'): 1,...</td>\n",
       "      <td>{('great', 'thing', 'place'): 1, ('thing', 'pl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>{'service': 7, 'amazing': 4, 'experience': 3, ...</td>\n",
       "      <td>{('dining', 'experience'): 2, ('demand', 'rive...</td>\n",
       "      <td>{('demand', 'river', 'floor'): 1, ('river', 'f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>{'dining': 4, 'experience': 3, 'art': 3, 'love...</td>\n",
       "      <td>{('dining', 'experience'): 2, ('ten', 'board')...</td>\n",
       "      <td>{('ten', 'board', 'honestly'): 1, ('board', 'h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>{'food': 4, 'server': 3, 'great': 3, 'alliance...</td>\n",
       "      <td>{('alliance', 'great'): 2, ('service', 'recent...</td>\n",
       "      <td>{('service', 'recent', 'server'): 1, ('recent'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>{'service': 4, 'friendly': 3, 'course': 3, 're...</td>\n",
       "      <td>{('favorite', 'restaurant'): 2, ('kid', 'frien...</td>\n",
       "      <td>{('kid', 'friendly', 'actually'): 1, ('friendl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>{'service': 5, 'food': 3, 'alliance': 3, 'amaz...</td>\n",
       "      <td>{('first', 'time'): 1, ('time', 'vincent'): 1,...</td>\n",
       "      <td>{('first', 'time', 'vincent'): 1, ('time', 'vi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>62 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            word_freq  \\\n",
       "1   {'time': 5, 'service': 3, 'food': 3, 'amazing'...   \n",
       "2   {'food': 6, 'service': 4, 'life': 3, 'heart': ...   \n",
       "3   {'food': 6, 'service': 5, 'time': 4, 'great': ...   \n",
       "4   {'restaurant': 3, 'food': 3, 'pm': 3, 'great':...   \n",
       "5   {'service': 3, 'food': 3, 'experience': 3, 'go...   \n",
       "..                                                ...   \n",
       "58  {'service': 7, 'amazing': 4, 'experience': 3, ...   \n",
       "59  {'dining': 4, 'experience': 3, 'art': 3, 'love...   \n",
       "60  {'food': 4, 'server': 3, 'great': 3, 'alliance...   \n",
       "61  {'service': 4, 'friendly': 3, 'course': 3, 're...   \n",
       "62  {'service': 5, 'food': 3, 'alliance': 3, 'amaz...   \n",
       "\n",
       "                                          bigram_freq  \\\n",
       "1   {('top', 'tier'): 2, ('amazing', 'service'): 1...   \n",
       "2   {('heart', 'belly'): 1, ('belly', 'happily'): ...   \n",
       "3   {('first', 'time'): 2, ('food', 'good'): 2, ('...   \n",
       "4   {('thoughtful', 'son'): 1, ('son', 'generous')...   \n",
       "5   {('great', 'thing'): 1, ('thing', 'place'): 1,...   \n",
       "..                                                ...   \n",
       "58  {('dining', 'experience'): 2, ('demand', 'rive...   \n",
       "59  {('dining', 'experience'): 2, ('ten', 'board')...   \n",
       "60  {('alliance', 'great'): 2, ('service', 'recent...   \n",
       "61  {('favorite', 'restaurant'): 2, ('kid', 'frien...   \n",
       "62  {('first', 'time'): 1, ('time', 'vincent'): 1,...   \n",
       "\n",
       "                                         trigram_freq  \n",
       "1   {('amazing', 'service', 'attentive'): 1, ('ser...  \n",
       "2   {('heart', 'belly', 'happily'): 1, ('belly', '...  \n",
       "3   {('review', 'private', 'party'): 1, ('private'...  \n",
       "4   {('thoughtful', 'son', 'generous'): 1, ('son',...  \n",
       "5   {('great', 'thing', 'place'): 1, ('thing', 'pl...  \n",
       "..                                                ...  \n",
       "58  {('demand', 'river', 'floor'): 1, ('river', 'f...  \n",
       "59  {('ten', 'board', 'honestly'): 1, ('board', 'h...  \n",
       "60  {('service', 'recent', 'server'): 1, ('recent'...  \n",
       "61  {('kid', 'friendly', 'actually'): 1, ('friendl...  \n",
       "62  {('first', 'time', 'vincent'): 1, ('time', 'vi...  \n",
       "\n",
       "[62 rows x 3 columns]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_frequent_words = {}\n",
    "\n",
    "def get_frequency(restaurant_id):\n",
    "\n",
    "    # Word Frequency Analysis\n",
    "    all_words = [word for tokens in df[df['restaurant_id'] == restaurant_id]['tokens'] for word in tokens]\n",
    "    word_freq = Counter(all_words)\n",
    "\n",
    "    # Sort the word frequency in descending order\n",
    "    word_freq = dict(sorted(word_freq.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "    # N-gram Analysis\n",
    "    bigrams = ngrams(all_words, 2)\n",
    "    bigram_freq = Counter(bigrams)\n",
    "\n",
    "    # Sort the bigram frequency in descending order\n",
    "    bigram_freq = dict(sorted(bigram_freq.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "    # Tri-gram Analysis\n",
    "    trigrams = ngrams(all_words, 3)\n",
    "    trigram_freq = Counter(trigrams)\n",
    "\n",
    "    # Sort the trigram frequency in descending order\n",
    "    trigram_freq = dict(sorted(trigram_freq.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "    return [word_freq, bigram_freq, trigram_freq]\n",
    "\n",
    "# We create a dictionary with the restaurant_id as key and the frequency of words, bigrams and trigrams as values\n",
    "for restaurant_id in df['restaurant_id']:\n",
    "    review_frequent_words[restaurant_id] = get_frequency(restaurant_id)\n",
    "\n",
    "review_frequent_words_df = pd.DataFrame.from_dict(review_frequent_words, orient='index', columns=['word_freq', 'bigram_freq', 'trigram_freq'])\n",
    "review_frequent_words_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "com3: We regroup similar words together to form topics. This will be useful for our users who will be able to choose the topic that is more important to them. We'll then choose the reviews that have the specific tokens found in the topics chosen to give the user the restaurant with the criteria they wish to have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/Users/alexandrecogordan/miniconda3/envs/tensorflow/lib/python3.10/site-packages/sklearn/manifold/_mds.py:298: FutureWarning: The default value of `normalized_stress` will change to `'auto'` in version 1.4. To suppress this warning, manually set the value of `normalized_stress`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el25937118253751369018215834\" style=\"background-color:white;\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el25937118253751369018215834_data = {\"mdsDat\": {\"x\": [0.3635982036162687, 0.0840649231052934, -0.29596587601543817, -0.19208391118109056, 0.11734430441347235, -0.0674391514711469, -0.09036818895408212, -0.27300743102522446, 0.26367997861414505, 0.09017714889780276], \"y\": [0.1755303666822016, 0.3158921007059202, -0.19745888449771265, 0.28510845290533443, -0.30678555863438556, 0.09675399214495621, -0.2614379186010089, 0.030372725487673658, -0.13148136400268692, -0.006493912190292214], \"topics\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \"cluster\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \"Freq\": [26.100876626498714, 14.47717652271383, 13.571005494446306, 10.515603029141051, 7.731031519728666, 7.154518662105682, 6.4225901184631, 5.759305332164477, 5.648671712126104, 2.6192209826120734]}, \"tinfo\": {\"Term\": [\"service\", \"food\", \"dinner\", \"table\", \"night\", \"time\", \"french\", \"great\", \"good\", \"speak\", \"amazing\", \"service\", \"wine\", \"wonderful\", \"great\", \"delicious\", \"dining\", \"meal\", \"well\", \"salad\", \"food\", \"experience\", \"good\", \"alliance\", \"restaurant\", \"dinner\", \"night\", \"overall\", \"date\", \"exceptional\", \"hotel\", \"lunch\", \"back\", \"always\", \"olive\", \"place\", \"spot\", \"really\", \"french\", \"escargot\", \"inside\", \"area\", \"first\", \"week\", \"fun\", \"waiter\", \"rude\", \"upside\", \"celebration\", \"time\", \"friend\", \"birthday\", \"menu\", \"restaurant\", \"nice\", \"speak\", \"soup\", \"onion\", \"life\", \"party\", \"town\", \"option\", \"street\", \"urinary\", \"full\", \"french\", \"restaurant\", \"attentive\", \"way\", \"high\", \"implacable\", \"incredible\", \"actually\", \"event\", \"close\", \"disappointed\", \"kid\", \"atmosphere\", \"price\", \"friendly\", \"really\", \"server\", \"super\", \"parking\", \"many\", \"surprised\", \"truly\", \"small\", \"absence\", \"mood\", \"copy\", \"chair\", \"friendly\", \"staff\", \"favorite\", \"lovely\", \"family\", \"fantastic\", \"absolutely\", \"brasserie\", \"wall\", \"guy\", \"anywhere\", \"cave\", \"warm\", \"class\", \"restaurant\", \"table\", \"kiss\", \"still\", \"door\", \"bread\", \"minute\", \"late\", \"ever\", \"selection\", \"perfectly\", \"old\", \"reservation\", \"course\", \"bartender\", \"tartar\", \"star\", \"bit\", \"second\", \"glass\", \"new\", \"etouffee\", \"third\", \"last\", \"review\", \"tonight\", \"overdue\", \"son\", \"downside\", \"wife\", \"even\", \"michel\", \"pantry\", \"bite\", \"year\", \"last\"], \"Freq\": [163.0, 160.0, 79.0, 36.0, 55.0, 59.0, 66.0, 74.0, 94.0, 34.0, 56.04580775316579, 160.6125256668109, 39.59577827307666, 35.092491616706496, 73.47694795636967, 33.15538714715874, 34.725718363840976, 25.39451778786477, 24.063558790017737, 18.918843997661373, 152.91339056653507, 58.0375869764513, 86.23391094422271, 46.272203586490896, 42.658613747807046, 78.58133130068204, 55.02267559645757, 26.255377714642307, 18.02246961196138, 16.4850712747459, 16.407012321491617, 15.54736472857519, 15.243240478309113, 12.826444871308345, 11.862357610588957, 52.0089761359348, 19.485999970010617, 29.661690001848672, 30.54248041604954, 20.57001365409615, 19.313594274205155, 18.983832020051985, 35.280845824566576, 14.353326736749823, 12.88136026934777, 11.752171441688944, 10.731824094785315, 10.031960623196179, 9.904493610239044, 54.21733469829305, 30.945964357811594, 34.82763870802305, 24.789428103312158, 33.314531165467244, 15.39955262358253, 34.21628434102262, 22.286657388471752, 22.034826867768643, 15.994149282469754, 14.257864723691569, 12.415526092377991, 12.067248158690967, 11.124845874051694, 10.97982387558697, 10.87814201871132, 35.486679442652026, 15.635348571734758, 14.683202079272936, 13.558814943823528, 12.229110636136935, 11.511240971818692, 11.460432450911343, 8.219545410305015, 8.201172674435487, 7.800167552320955, 7.408753794235685, 6.890898054382434, 25.77866525621714, 11.326126061286617, 13.21953122651048, 14.481938389685903, 9.702685129671021, 14.41520149080187, 13.002494768118405, 12.554587831269147, 11.976240820515022, 10.36325667527726, 9.563941927299076, 9.437543121440282, 9.311963161223888, 9.243653509668487, 8.690363175571802, 20.385254336760653, 10.082106635038041, 24.197859593384752, 14.497413932196183, 13.707558337093799, 11.206187796791504, 10.44283806606024, 7.166749361849157, 7.109286221904548, 6.160274025153228, 5.730241079381713, 5.730241079381713, 10.004285561023988, 6.913209823465937, 13.1377645376629, 35.395746386893656, 18.00814863248028, 10.221507261323499, 7.006498679416142, 6.876431390624967, 5.681536516152279, 5.455754738248063, 5.244380390663356, 4.53848845201928, 3.737488059161451, 9.75556546194125, 6.862941867060388, 24.839513639032713, 16.25505350923533, 15.101305340813113, 14.03986953128102, 12.318498970927648, 9.746946007595112, 8.347124181512916, 7.327674149461561, 6.505935779130338, 6.505935779130338, 13.534290472475778, 9.158602135131236, 6.954632489253615, 5.533281420182537, 5.3079111326582105, 5.145906179005374, 3.2234719144757245, 5.894664925277464, 1.7090294283247345, 1.4263556520083471, 1.3410080947404457, 8.61634373206487, 2.3829309127944627], \"Total\": [163.0, 160.0, 79.0, 36.0, 55.0, 59.0, 66.0, 74.0, 94.0, 34.0, 56.959842565502925, 163.27581273227892, 40.261011953926264, 35.75772263216519, 74.93277478922933, 33.820697896027305, 35.512214366605505, 26.123258320313784, 24.896821685235043, 19.584156488234367, 160.12670164092827, 61.20691112341506, 94.42368789063555, 56.756000479388554, 114.71544036513814, 79.23902107396832, 55.74981753266961, 26.91319137022075, 18.680074122522594, 17.142710203825853, 17.064652584982557, 16.211171394531856, 15.900867928345287, 13.484099585329105, 12.52056021392359, 62.60586732380362, 23.364419936695807, 51.13407072337466, 66.60352601417136, 21.24177376645342, 19.98513270891954, 19.655276347679127, 36.87977293379005, 15.024763882227006, 13.552814078804456, 12.424049629829073, 11.403522927007867, 10.703968078551364, 10.575883368750304, 59.89593714076329, 34.79445804898186, 40.33539464821134, 38.90674858253107, 114.71544036513814, 38.01350344368751, 34.88812385889941, 22.95832410403468, 22.706497383901887, 16.665868758952836, 14.929804436198278, 13.087469708949286, 12.739291709952079, 11.796799203808169, 11.651919317055789, 11.550236476944743, 66.60352601417136, 114.71544036513814, 15.369329567542646, 14.24491237645026, 12.915210486790372, 12.19739128055387, 12.146545137550692, 8.90565328935852, 8.887277126871927, 8.486428873007819, 8.09502445816336, 7.576988726720685, 28.640804702144877, 18.985435119425407, 34.20865862838627, 51.13407072337466, 22.923534922923253, 15.087961837300265, 13.675453169394194, 13.227486282655995, 12.649294785405308, 11.036023927416595, 10.236723008302016, 10.11023485310391, 9.985028400525028, 9.916410733362587, 9.36333010045869, 34.20865862838627, 21.242818457115927, 24.89556126667005, 15.195133779160642, 14.405247686530748, 11.903958065733454, 11.14071556190928, 7.864486974169727, 7.806977213402032, 6.858155464819546, 6.427900056688403, 6.427900056688403, 12.71843263045073, 9.589445402720038, 114.71544036513814, 36.07490422772225, 18.687209679034556, 10.900526185711135, 7.685523103318759, 7.555442174860984, 6.360590366668678, 6.134806872007579, 5.923407904281754, 5.217559428745713, 4.4165269094611626, 13.595179113027205, 20.551446795475467, 25.529423953167253, 16.945250040742312, 15.791259711679006, 14.729813389002931, 13.008566500074132, 10.43689918076722, 9.037069144975355, 8.01763966866982, 7.1958669475534025, 7.1958669475534025, 16.555720051345663, 9.862135394410673, 7.65828416542789, 6.236834831876114, 6.011926276874485, 5.8495204708059365, 3.9269900113134852, 7.502054978455288, 2.412531587710102, 2.130033932535053, 2.0446506907981816, 16.846717328390767, 16.555720051345663], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\"], \"logprob\": [10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -3.349, -2.2961, -3.6964, -3.8171, -3.0782, -3.8739, -3.8277, -4.1406, -4.1944, -4.435, -2.3453, -3.314, -2.9181, -3.5406, -3.6219, -2.4216, -2.778, -3.5179, -3.8941, -3.9833, -3.988, -4.0418, -4.0616, -4.2342, -4.3124, -2.8343, -3.816, -3.3959, -3.3666, -3.6973, -3.7603, -3.7775, -3.1578, -4.0571, -4.1653, -4.2571, -4.3479, -4.4153, -4.4281, -2.7281, -3.2889, -3.1707, -3.5107, -3.2151, -3.9868, -2.9333, -3.362, -3.3734, -3.6938, -3.8087, -3.9471, -3.9755, -4.0568, -4.07, -4.0793, -2.8969, -3.7165, -3.4717, -3.5514, -3.6546, -3.7151, -3.7195, -4.0519, -4.0541, -4.1043, -4.1557, -4.2282, -2.9089, -3.7313, -3.5767, -3.4855, -3.886, -3.4126, -3.5158, -3.5508, -3.598, -3.7426, -3.8229, -3.8362, -3.8496, -3.857, -3.9187, -3.0661, -3.7701, -2.7867, -3.299, -3.355, -3.5565, -3.6271, -4.0035, -4.0116, -4.1549, -4.2272, -4.2272, -3.67, -4.0396, -3.3975, -2.2974, -2.9732, -3.5395, -3.9171, -3.9359, -4.1268, -4.1673, -4.2068, -4.3514, -4.5456, -3.5861, -3.9378, -2.6322, -3.0562, -3.1298, -3.2027, -3.3335, -3.5676, -3.7227, -3.8529, -3.9719, -3.9719, -3.2394, -2.8613, -3.1366, -3.3653, -3.4068, -3.4378, -3.9056, -3.302, -4.5401, -4.7209, -4.7826, -2.9224, -4.2077], \"loglift\": [10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.327, 1.3268, 1.3265, 1.3244, 1.3236, 1.3233, 1.3208, 1.3149, 1.3092, 1.3086, 1.2971, 1.29, 1.2525, 1.139, 0.354, 1.9243, 1.9195, 1.9079, 1.8968, 1.8935, 1.8933, 1.8908, 1.8904, 1.8826, 1.8786, 1.7472, 1.7511, 1.388, 1.153, 1.9651, 1.9631, 1.9625, 1.9529, 1.9515, 1.9464, 1.9416, 1.9365, 1.9324, 1.9316, 1.8976, 1.88, 1.8504, 1.5465, 0.7608, 1.0936, 2.2329, 2.2226, 2.2223, 2.2112, 2.2063, 2.1996, 2.1981, 2.1937, 2.1929, 2.1924, 1.6227, 0.2594, 2.5143, 2.5106, 2.5053, 2.502, 2.5018, 2.4798, 2.4796, 2.4756, 2.4713, 2.465, 2.4546, 2.0434, 1.6091, 1.2984, 1.7002, 2.5918, 2.587, 2.5852, 2.5827, 2.5745, 2.5694, 2.5686, 2.5676, 2.5672, 2.5628, 2.1198, 1.8922, 2.7169, 2.6983, 2.6957, 2.6849, 2.6807, 2.6524, 2.6517, 2.638, 2.6305, 2.6305, 2.5053, 2.4181, 0.5784, 2.8353, 2.8173, 2.79, 2.7619, 2.7602, 2.7415, 2.737, 2.7326, 2.7149, 2.6874, 2.5225, 1.7576, 2.8464, 2.8322, 2.8291, 2.8258, 2.8192, 2.8054, 2.7943, 2.7838, 2.773, 2.773, 2.6722, 3.5683, 3.5459, 3.5226, 3.5177, 3.5141, 3.4449, 3.4012, 3.2975, 3.2413, 3.2205, 2.9718, 1.7039]}, \"token.table\": {\"Topic\": [6, 7, 5, 1, 2, 3, 7, 2, 1, 7, 3, 5, 8, 5, 2, 9, 3, 7, 9, 10, 7, 8, 7, 3, 6, 7, 9, 5, 6, 9, 2, 1, 1, 2, 5, 8, 10, 3, 9, 6, 10, 5, 8, 2, 1, 2, 7, 7, 7, 3, 10, 1, 2, 5, 2, 4, 3, 4, 5, 6, 4, 3, 9, 1, 4, 8, 1, 10, 7, 5, 2, 5, 5, 3, 5, 8, 9, 10, 8, 4, 7, 2, 6, 1, 2, 3, 4, 6, 10, 8, 6, 9, 1, 3, 4, 5, 6, 2, 8, 9, 2, 4, 4, 2, 10, 10, 6, 4, 8, 2, 5, 6, 7, 8, 10, 3, 5, 1, 2, 4, 5, 3, 8, 1, 2, 3, 4, 7, 9, 10, 3, 1, 9, 8, 1, 5, 1, 5, 6, 10, 4, 4, 2, 8, 1, 2, 6, 9, 8, 4, 6, 6, 8, 9, 9, 3, 7, 10, 4, 6, 3, 4, 3, 7, 4, 7, 5, 3, 1, 10, 1, 1, 3, 9, 10], \"Freq\": [0.8901870362820444, 0.8976084116347562, 0.8983057997058231, 0.8104869901237193, 0.05285784718198169, 0.0704771295759756, 0.05285784718198169, 0.9640984863493731, 0.9831487847881756, 0.9334308167652419, 0.9666615550914652, 0.9077957225850184, 0.06983044019884757, 0.9759697021317959, 0.9433447323501519, 0.9442174037875156, 0.8677242482751327, 0.12396060689644754, 0.9224690514463385, 0.48908109561228985, 0.8900771306495815, 0.9264844913102385, 0.9334308167652419, 0.9455474924721723, 0.9611964870873354, 0.7299692219963477, 0.20856263485609935, 0.942681558958802, 0.9075864485645565, 0.9792622052836577, 0.9635936068528429, 0.9757338568662799, 0.985576388976544, 0.9969835433258925, 0.8647287029430663, 0.9108033254076439, 0.8547709209591173, 0.9886180048280503, 0.9727806324128885, 0.1332968103901986, 0.7997808623411916, 0.9001632204998851, 0.8441086754106086, 0.9333413334158314, 0.9476054082038419, 0.03267604855875317, 0.9718680514664344, 0.9240623949830962, 0.9640272714851775, 0.9490297042456093, 0.02711513440701741, 0.955493358897073, 0.03747032779988521, 0.006245054633314202, 0.4654408235594624, 0.5254977040187478, 0.8909464822346072, 0.0862206273130265, 0.3800207468296529, 0.5846473028148507, 0.9523614535474609, 0.9592103842353291, 0.8852427564359215, 0.9107884040666562, 0.06354337702790624, 0.010590562837984374, 0.9742065498753272, 0.013345295203771606, 0.8748708061195685, 0.9291370057246496, 0.9376106498693395, 0.9838169264219171, 0.9056073044172716, 0.9507067216781669, 0.9238498633783233, 0.9632256666009615, 0.8456291817317887, 0.12080416881882695, 0.8150215816596326, 0.9600459616847076, 0.9213475974262426, 0.9869737115602216, 0.9828020019983481, 0.9570015996266314, 0.23132233681539127, 0.6425620467094202, 0.025702481868376807, 0.10280992747350723, 0.8290046895917894, 0.943308663837515, 0.901349464316673, 0.8730749059917963, 0.23675797242241645, 0.3945966207040274, 0.13153220690134246, 0.026306441380268494, 0.18414508966187945, 0.9865503141381545, 0.7355548549130755, 0.22066645647392263, 0.9584235685121584, 0.9688856730319504, 0.9419675970388106, 0.9660690046877461, 0.9620264383681184, 0.4694760889606361, 0.9506083519845715, 0.9377215930609307, 0.9056890362042466, 0.8305930773397157, 0.06389177517997813, 0.06389177517997813, 0.015972943794994532, 0.015972943794994532, 0.015972943794994532, 0.3687036908012596, 0.579391514116265, 0.07822572980037554, 0.5866929735028166, 0.05866929735028166, 0.2737900543013144, 0.6325588718582107, 0.3406086233082673, 0.37484056080969935, 0.07845500109970452, 0.2876683373655832, 0.1394755575105858, 0.11332389047735096, 0.008717222344411612, 0.9125812656253649, 0.9646141872480327, 0.9701719862897689, 0.9581389861873607, 0.9583024531456052, 0.5671027633264432, 0.43623289486649475, 0.9860615439960446, 0.012249211726658939, 0.9768751183254608, 0.8316801919599435, 0.9582580984704251, 0.9745436624081215, 0.8132022986865977, 0.1284003629505154, 0.1412241980063177, 0.329523128681408, 0.4707473266877257, 0.9504533173822962, 0.917386906799822, 0.9324563222580791, 0.9278920606353458, 0.9486694873966839, 0.9702035459072341, 0.9498925528345404, 0.9727806324128885, 0.9015636548618137, 0.08347811619090867, 0.9140428650585193, 0.9169075663108761, 0.9061234431684386, 0.9342329803877146, 0.9440504779241369, 0.9658686464990476, 0.8966338454252543, 0.15725208114178782, 0.7862604057089391, 0.9828070282232728, 0.9317950092088161, 0.9639784669475743, 0.7639438835742215, 0.9935170046340375, 0.9788095388523542, 0.17807623535917463, 0.23743498047889952, 0.5342287060775239], \"Term\": [\"absence\", \"absolutely\", \"actually\", \"alliance\", \"alliance\", \"alliance\", \"alliance\", \"always\", \"amazing\", \"anywhere\", \"area\", \"atmosphere\", \"atmosphere\", \"attentive\", \"back\", \"bartender\", \"birthday\", \"birthday\", \"bit\", \"bite\", \"brasserie\", \"bread\", \"cave\", \"celebration\", \"chair\", \"class\", \"class\", \"close\", \"copy\", \"course\", \"date\", \"delicious\", \"dining\", \"dinner\", \"disappointed\", \"door\", \"downside\", \"escargot\", \"etouffee\", \"even\", \"even\", \"event\", \"ever\", \"exceptional\", \"experience\", \"experience\", \"family\", \"fantastic\", \"favorite\", \"first\", \"first\", \"food\", \"food\", \"food\", \"french\", \"french\", \"friend\", \"friend\", \"friendly\", \"friendly\", \"full\", \"fun\", \"glass\", \"good\", \"good\", \"good\", \"great\", \"great\", \"guy\", \"high\", \"hotel\", \"implacable\", \"incredible\", \"inside\", \"kid\", \"kiss\", \"last\", \"last\", \"late\", \"life\", \"lovely\", \"lunch\", \"many\", \"meal\", \"menu\", \"menu\", \"menu\", \"menu\", \"michel\", \"minute\", \"mood\", \"new\", \"nice\", \"nice\", \"nice\", \"nice\", \"nice\", \"night\", \"old\", \"old\", \"olive\", \"onion\", \"option\", \"overall\", \"overdue\", \"pantry\", \"parking\", \"party\", \"perfectly\", \"place\", \"place\", \"place\", \"place\", \"place\", \"place\", \"price\", \"price\", \"really\", \"really\", \"really\", \"really\", \"reservation\", \"reservation\", \"restaurant\", \"restaurant\", \"restaurant\", \"restaurant\", \"restaurant\", \"restaurant\", \"review\", \"rude\", \"salad\", \"second\", \"selection\", \"server\", \"server\", \"service\", \"service\", \"small\", \"son\", \"soup\", \"speak\", \"spot\", \"spot\", \"staff\", \"staff\", \"staff\", \"star\", \"still\", \"street\", \"super\", \"surprised\", \"table\", \"tartar\", \"third\", \"time\", \"time\", \"tonight\", \"town\", \"truly\", \"upside\", \"urinary\", \"waiter\", \"wall\", \"warm\", \"warm\", \"way\", \"week\", \"well\", \"wife\", \"wine\", \"wonderful\", \"year\", \"year\", \"year\"]}, \"R\": 10, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [10, 3, 2, 7, 5, 9, 1, 6, 4, 8]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el25937118253751369018215834\", ldavis_el25937118253751369018215834_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el25937118253751369018215834\", ldavis_el25937118253751369018215834_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el25937118253751369018215834\", ldavis_el25937118253751369018215834_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We convert the tokens into tuples where we'll have the word index (its placement on the map) and its frequency\n",
    "id2word = corpora.Dictionary(df['tokens'])\n",
    "corpus = [id2word.doc2bow(text) for text in df['tokens']]\n",
    "\n",
    "# We create the LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                            id2word=id2word,\n",
    "                                            num_topics=10,\n",
    "                                            random_state=100,\n",
    "                                            update_every=1,\n",
    "                                            chunksize=100,\n",
    "                                            passes=10,\n",
    "                                            alpha='auto',\n",
    "                                            per_word_topics=True)\n",
    "\n",
    "# We visualise the topics\n",
    "pyLDAvis.enable_notebook(local=True)\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word, mds='mmds', R=10)\n",
    "pyLDAvis.display(vis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "com4: The visualization is well-constructed. It provides us with a high-level overview of the topics and allows for an interactive exploration of the terms that define each topic.\n",
    "The map shows a good separation of topics which suggests distinct themes with salient terms captured by the model.\n",
    "The presence of both common and specific terms within the topics suggests the model is effectively capturing the range of discourse present in the data which we usually find in reviews.\n",
    "The distance between each theme suggests that the themes are diverse yet distinct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using themes\n",
    "\n",
    "com5: We proceed to create a dictionary to rename the topics based on the top 5 keywords of each topic. This will allow us to classify the review into themes that the user will be able to use. This will work by looking to see if the reviews contains the corresponding keywords of the topics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_distribution(lda_model, bow):\n",
    "    return lda_model.get_document_topics(bow, minimum_probability=0)\n",
    "\n",
    "# We get the topic distribution for each review\n",
    "df['topic_distribution'] = [get_topic_distribution(lda_model, corpus[i]) for i in range(len(df))]\n",
    "\n",
    "# We get the top topics for each review\n",
    "def get_top_topics(topic_distribution, num_topics=5):\n",
    "    # Sort the topics by probability and select the top ones\n",
    "    return sorted(topic_distribution, key=lambda x: x[1], reverse=True)[:num_topics]\n",
    "\n",
    "df['top_topics'] = df['topic_distribution'].apply(lambda x: get_top_topics(x, 11 - 1))\n",
    "\n",
    "# We label the topics\n",
    "def label_topics(topic_list, lda_model):\n",
    "    labels = []\n",
    "    for topic_id, _ in topic_list:\n",
    "        # Get the top words in the topic\n",
    "        words = lda_model.show_topic(topic_id, 10)\n",
    "        # Create a label (e.g., by joining the top words)\n",
    "        label = [word for word, prob in words]\n",
    "        labels.append(label)\n",
    "    return labels\n",
    "\n",
    "# We create a dictionary with the labels as keys and the words as values\n",
    "def topicise(labels, label_dict):\n",
    "    topics = []\n",
    "\n",
    "    for topic_list in labels:\n",
    "        for key, value in label_dict.items():\n",
    "            if set(value).issubset(set(topic_list)):\n",
    "                topics.append(key)\n",
    "\n",
    "    return topics\n",
    "\n",
    "# We label the topics\n",
    "label_dict = {\n",
    "    'Quality of Food & Service' : ['service', 'food', 'experience', 'alliance', 'wine'],\n",
    "    'Occasion Type' : ['dinner', 'night', 'place', 'spot', 'date'],\n",
    "    'Personal Celebrations' : ['time', 'first', 'birthday', 'friend', 'menu'],\n",
    "    'French Cuisine Essentials' : ['french', 'speak', 'onion', 'soup', 'life'],\n",
    "    'Ambience & Service' : ['atmosphere', 'attentive', 'implacable', 'server', 'price'],\n",
    "    'General Comfort' : ['friendly', 'absence', 'parking', 'staff', 'mood'],\n",
    "    'Dining Experience' : ['favorite', 'lovely', 'family', 'warm', 'brasserie'],\n",
    "    'Restaurant Environment' : ['table', 'kiss', 'old', 'door', 'bread'], \n",
    "    'Menu' : ['course', 'bartender', 'tartar', 'star', 'glass'],\n",
    "    'General Reviews' : ['review', 'year', 'tonight', 'overdue', 'downside']\n",
    "}\n",
    "\n",
    "df['top_topic_labels'] = df['top_topics'].apply(lambda x: label_topics(x, lda_model))\n",
    "df['topics'] = df['top_topic_labels'].apply(lambda x: topicise(x, label_dict))\n",
    "df.drop(columns=['topic_distribution', 'top_topics'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT - Sentiment analysis & Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "com6: We've decided to go with this: BERT for Text Classification: Given its context-aware nature and superior performance in understanding nuances, BERT is generally the better choice for text classification.\n",
    "Word2Vec for Semantic Search: For semantic search tasks, Word2Vec's efficiency and effectiveness in finding similar words make it a strong choice.\n",
    "We also used BERT in our advantage to generate new reviews and fix the imbalance issue we have with our reviews. From our experiments, we've realised that the training process for our word2vec and td-idf model was skewed. This was caused by the large amount of initial positive reviews compared to the initial neutral and negative reviews. Therefore, we've used generation by using a context augmenter on our previously inserted negative and neutral reviews. This deeply improved the cohesion of our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer and model\n",
    "tokeniser = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
    "\n",
    "# Sentiment analysis function\n",
    "def sentiment_score(review):\n",
    "    tokens = tokeniser.encode(review, return_tensors=\"pt\")\n",
    "    result = model(tokens)\n",
    "    return int(torch.argmax(result.logits))+1\n",
    "\n",
    "# Application to the DataFrame\n",
    "df['sentiment'] = df['text'].apply(lambda x: sentiment_score(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "com7: We've carried out this sentiment analysis by using the twitter-roberta-base-sentiment model. This will ouput values as such:\n",
    "* 1: negative sentiment\n",
    "* 2: neutral sentiment\n",
    "* 3: positive sentiment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're going to incrase the number of negative and neutral reviews to balance the dataset using text-augmentation and back-translation.\n",
    "context_aug = naw.ContextualWordEmbsAug(model_path='bert-base-uncased', action=\"substitute\", aug_p=0.2)\n",
    "\n",
    "# Translation pipelines for back-translation\n",
    "english_to_french = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-fr\")\n",
    "french_to_english = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-fr-en\")\n",
    "\n",
    "# Back-translation function\n",
    "def back_translate(text):\n",
    "    # Translate to French\n",
    "    french_text = english_to_french(text)[0]['translation_text']\n",
    "    # Translate back to English\n",
    "    back_translated_text = french_to_english(french_text)[0]['translation_text']\n",
    "    return back_translated_text\n",
    "\n",
    "def generate_new_reviews(text):\n",
    "    # text = back_translate(text)\n",
    "    text = context_aug.augment(text)\n",
    "    # text = back_translate(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "com8: Back-translation is another way to rephrase texts without losing context. However, after experimenting, we realised that the texts were too similar if back-translation was used by itself and it strayed too far from its context if it was used in complement to context augmentation. Therefore, we judged that using context augmented generation of our reviews gave us sufficient results to generate new reviews with it by itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a function to push the new reviews to the DataFrame\n",
    "\n",
    "def push_new_reviews(df):\n",
    "    # We record the original data types to conserve them between concatenations\n",
    "    original_dtypes = df.dtypes\n",
    "\n",
    "    # We create a DataFrame for each sentiment\n",
    "    sentiment_1_rows = df[df['sentiment'] == df['sentiment'].value_counts().index[1]]\n",
    "    sentiment_2_rows = df[df['sentiment'] == df['sentiment'].value_counts().index[2]]\n",
    "    \n",
    "    # We generate new reviews for each sentiment\n",
    "\n",
    "    # The sentiment with the most second amount of reviews (which was NEUTRAL in our case)\n",
    "    for i in range(df['sentiment'].value_counts().iloc[0] - df['sentiment'].value_counts().iloc[1] - 1):\n",
    "        random_row = random.choice(sentiment_1_rows.index)\n",
    "        target_row = df.loc[random_row].copy()  # Make a copy of the selected row\n",
    "        \n",
    "        new_review = generate_new_reviews(target_row['cleaned_text'])\n",
    "        target_row['cleaned_text'] = preprocessing(new_review[0])\n",
    "\n",
    "        # Convert target_row to DataFrame and transpose\n",
    "        new_row_df = pd.DataFrame(target_row).transpose()\n",
    "\n",
    "        # Set data types explicitly\n",
    "        for col in new_row_df.columns:\n",
    "            new_row_df[col] = new_row_df[col].astype(original_dtypes[col])\n",
    "\n",
    "        df = pd.concat([df, new_row_df], ignore_index=True)\n",
    "\n",
    "    # The sentiment with the least amount of reviews (which was NEGATIVE in our case)\n",
    "    for i in range(df['sentiment'].value_counts().iloc[0] - df['sentiment'].value_counts().iloc[2] - 1):\n",
    "        random_row = random.choice(sentiment_2_rows.index)\n",
    "        target_row = df.loc[random_row].copy()  # Make a copy of the selected row\n",
    "        \n",
    "        new_review = generate_new_reviews(target_row['cleaned_text'])\n",
    "        target_row['cleaned_text'] = preprocessing(new_review[0])\n",
    "\n",
    "        # Convert target_row to DataFrame and transpose\n",
    "        new_row_df = pd.DataFrame(target_row).transpose()\n",
    "\n",
    "        # Set data types explicitly\n",
    "        for col in new_row_df.columns:\n",
    "            new_row_df[col] = new_row_df[col].astype(original_dtypes[col])\n",
    "\n",
    "        df = pd.concat([df, new_row_df], ignore_index=True)\n",
    "        \n",
    "    return df\n",
    "\n",
    "generated_df = push_new_reviews(df)\n",
    "df = generated_df.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Word2vec model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the Phrases algorithm to detect common phrases (bigrams and trigrams) from a stream of sentences. The Phrases model is then used to transform sentences by replacing common phrases with a common token, e.g. 'new','york','times' -> 'new_york_times'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phrases alogrithm - bigrams and trigrams \n",
    "\n",
    "# We've set the parameters to 3 and 5 respectively for the minimum count and threshold because we want to keep the bigrams and trigrams that appear at least 3 times and have a score of at least 5. This was done after several iterations of the model and by looking to see if the grouped tokens made sense in a restaurant review context.\n",
    "\n",
    "min_count = 3\n",
    "threshold = 5\n",
    "\n",
    "phrases = Phrases(df['tokens'], min_count=min_count, threshold=threshold)\n",
    "phraser = Phraser(phrases)\n",
    "\n",
    "df['bigrams'] = phraser[ df['tokens']]\n",
    "df['trigrams'] = phraser[df['bigrams']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train our Word2Vec model with the tokens from our reviews. We didn't end up using the bigrams and trigrams because they didn't improve the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_size = 300 # Dimensionality of the word vectors\n",
    "window = 8 # Maximum distance between the current and predicted word within a sentence\n",
    "min_count = 10 # Ignores all words with total frequency lower than this\n",
    "workers = 8 # Number of CPU cores\n",
    "learning_rate = 0.001 # Initial learning rate\n",
    "sample = 1e-3 # Downsample setting for frequent words\n",
    "\n",
    "# Training the model\n",
    "word2vec_model = Word2Vec(sentences=df['tokens'], vector_size=vector_size, window=window, min_count=min_count, workers=workers, sg=1, alpha=learning_rate, sample=sample, epochs=20)\n",
    "\n",
    "# Save the model\n",
    "word2vec_model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 758.5/758.5MB downloaded\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'KeyedVectors' object has no attribute 'wv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[252], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m         model_res \u001b[38;5;241m=\u001b[39m model_res \u001b[38;5;241m/\u001b[39m ctr\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_res\n\u001b[0;32m---> 16\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvectors\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtokens\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mvectorize_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m X \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvectors\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto_list()\n\u001b[1;32m     19\u001b[0m y \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentiment\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto_list()\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/pandas/core/series.py:4760\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4625\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4626\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4627\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4632\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4633\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4634\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4635\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4636\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4751\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4752\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   4753\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4754\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4755\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4756\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4757\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4758\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4759\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m-> 4760\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/pandas/core/apply.py:1207\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1204\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[1;32m   1206\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[0;32m-> 1207\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/pandas/core/apply.py:1287\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1281\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[1;32m   1282\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[1;32m   1283\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[1;32m   1284\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[1;32m   1285\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[1;32m   1286\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1287\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1288\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[1;32m   1289\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1291\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1292\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1293\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1294\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/pandas/core/base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/pandas/core/algorithms.py:1814\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1812\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1813\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1814\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1815\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1816\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1817\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[1;32m   1818\u001b[0m     )\n",
      "File \u001b[0;32mlib.pyx:2917\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[252], line 16\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     13\u001b[0m         model_res \u001b[38;5;241m=\u001b[39m model_res \u001b[38;5;241m/\u001b[39m ctr\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_res\n\u001b[0;32m---> 16\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvectors\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mvectorize_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     18\u001b[0m X \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvectors\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto_list()\n\u001b[1;32m     19\u001b[0m y \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentiment\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto_list()\n",
      "Cell \u001b[0;32mIn[252], line 9\u001b[0m, in \u001b[0;36mvectorize_model\u001b[0;34m(sent, model)\u001b[0m\n\u001b[1;32m      7\u001b[0m ctr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m sent:\n\u001b[0;32m----> 9\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwv\u001b[49m\u001b[38;5;241m.\u001b[39mkey_to_index:\n\u001b[1;32m     10\u001b[0m         ctr \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     11\u001b[0m         model_res \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mwv[word]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'KeyedVectors' object has no attribute 'wv'"
     ]
    }
   ],
   "source": [
    "model = Word2Vec.load(\"word2vec.model\")\n",
    "# model = api.load('glove-twitter-200') # We could have used the glove model too to get similar results. We chose to use the word2vec model because it was trained on our own data.\n",
    "\n",
    "def vectorize_model(sent, model):\n",
    "    vector_size = model.vector_size\n",
    "    model_res = np.zeros(vector_size)\n",
    "    ctr = 0\n",
    "    for word in sent:\n",
    "        if word in model.wv.key_to_index:\n",
    "            ctr += 1\n",
    "            model_res += model.wv[word]\n",
    "    if ctr > 0:\n",
    "        model_res = model_res / ctr\n",
    "    return model_res\n",
    "\n",
    "df['vectors'] = df['tokens'].apply(lambda x: vectorize_model(x, model))\n",
    "\n",
    "X = df['vectors'].to_list()\n",
    "y = df['sentiment'].to_list()\n",
    "\n",
    "test_size = 0.3\n",
    "stratify_value = y\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size,stratify=stratify_value, random_state=42)\n",
    "\n",
    "# Create a list of classifiers\n",
    "classifiers = [\n",
    "    (\"Logistic Regression\", LogisticRegression()),\n",
    "    (\"SVM\", SVC(kernel='linear')),\n",
    "    (\"Random Forest\", RandomForestClassifier()),\n",
    "    (\"K-Nearest Neighbors\", KNeighborsClassifier()),\n",
    "    (\"Decision Tree\", DecisionTreeClassifier())\n",
    "]\n",
    "\n",
    "# Loop through the classifiers and test each one\n",
    "for name, classifier in classifiers:\n",
    "    print(f\"Testing {name} Classifier:\")\n",
    "    classifier.fit(X_train, y_train)  # Train the classifier on your data\n",
    "    y_pred = classifier.predict(X_test)  # Make predictions on the test data\n",
    "\n",
    "    # Evaluate the classifier using metrics of your choice (e.g., accuracy)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy: {accuracy:.4f}\\n\")\n",
    "\n",
    "# print(\"Logistic Regression Precision:\",metrics.precision_score(y_test, predicted, average='micro'))\n",
    "# print(\"Logistic Regression Recall:\",metrics.recall_score(y_test, predicted, average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "com6: We can see that the best model in our use case is the random forest classifier in terms of accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also apply other ways to evaluate our model - such as the F1 score, ROC curve and AUC score. We use the cross-validation method to evaluate the model's performance on unseen data and we use the grid search method to find the best parameters for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "\n",
      "[[145   0   0]\n",
      " [  0 139   5]\n",
      " [  0   1 144]]\n",
      "Cross-Validation Accuracy Scores: [0.97524752 0.96534653 1.         0.9950495  0.98514851]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "multiclass format is not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[249], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m recall \u001b[38;5;241m=\u001b[39m recall_score(y_test, y_pred, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweighted\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Calculate the false positive rate (fpr)\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m fpr, tpr, thresholds \u001b[38;5;241m=\u001b[39m \u001b[43mroc_curve\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Create a mappable object (e.g., an image or a contour set)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(confusion)\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:214\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    210\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    211\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    212\u001b[0m         )\n\u001b[1;32m    213\u001b[0m     ):\n\u001b[0;32m--> 214\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    224\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/sklearn/metrics/_ranking.py:1095\u001b[0m, in \u001b[0;36mroc_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight, drop_intermediate)\u001b[0m\n\u001b[1;32m    993\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[1;32m    994\u001b[0m     {\n\u001b[1;32m    995\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-like\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1004\u001b[0m     y_true, y_score, \u001b[38;5;241m*\u001b[39m, pos_label\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, drop_intermediate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1005\u001b[0m ):\n\u001b[1;32m   1006\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute Receiver operating characteristic (ROC).\u001b[39;00m\n\u001b[1;32m   1007\u001b[0m \n\u001b[1;32m   1008\u001b[0m \u001b[38;5;124;03m    Note: this implementation is restricted to the binary classification task.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1093\u001b[0m \u001b[38;5;124;03m    array([ inf, 0.8 , 0.4 , 0.35, 0.1 ])\u001b[39;00m\n\u001b[1;32m   1094\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1095\u001b[0m     fps, tps, thresholds \u001b[38;5;241m=\u001b[39m \u001b[43m_binary_clf_curve\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1096\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpos_label\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\n\u001b[1;32m   1097\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1099\u001b[0m     \u001b[38;5;66;03m# Attempt to drop thresholds corresponding to points in between and\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m     \u001b[38;5;66;03m# collinear with other points. These are always suboptimal and do not\u001b[39;00m\n\u001b[1;32m   1101\u001b[0m     \u001b[38;5;66;03m# appear on a plotted ROC curve (and thus do not affect the AUC).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1106\u001b[0m     \u001b[38;5;66;03m# but does not drop more complicated cases like fps = [1, 3, 7],\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m     \u001b[38;5;66;03m# tps = [1, 2, 4]; there is no harm in keeping too many thresholds.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m drop_intermediate \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(fps) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/sklearn/metrics/_ranking.py:804\u001b[0m, in \u001b[0;36m_binary_clf_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight)\u001b[0m\n\u001b[1;32m    802\u001b[0m y_type \u001b[38;5;241m=\u001b[39m type_of_target(y_true, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    803\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (y_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m (y_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m pos_label \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)):\n\u001b[0;32m--> 804\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m format is not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(y_type))\n\u001b[1;32m    806\u001b[0m check_consistent_length(y_true, y_score, sample_weight)\n\u001b[1;32m    807\u001b[0m y_true \u001b[38;5;241m=\u001b[39m column_or_1d(y_true)\n",
      "\u001b[0;31mValueError\u001b[0m: multiclass format is not supported"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, roc_auc_score, precision_recall_curve, average_precision_score\n",
    "\n",
    "confusion = confusion_matrix(y_test, y_pred)\n",
    "print('Confusion Matrix\\n')\n",
    "print(confusion)\n",
    "\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1: {f1:.4f}\")\n",
    "\n",
    "y_scores = classifier.predict_proba(X_test)[:, 1]\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_scores)\n",
    "auc = roc_auc_score(y_test, y_scores)\n",
    "\n",
    "print(f\"AUC: {auc:.4f}\")\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_scores)\n",
    "average_precision = average_precision_score(y_test, y_scores)\n",
    "\n",
    "print(f\"Average Precision: {average_precision:.4f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(confusion, cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.colorbar()\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.xticks([1, 2, 3], ['Negative', 'Neutral', 'Positive'])\n",
    "plt.yticks([1, 2, 3], ['Negative', 'Neutral', 'Positive'])\n",
    "plt.show()\n",
    "\n",
    "# ROC Curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, label='ROC Curve (AUC = {:.4f})'.format(auc))\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "# Precision-Recall Curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(recall, precision, label='Precision-Recall Curve (AP = {:.4f})'.format(average_precision))\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.legend(loc='lower left')\n",
    "plt.show()\n",
    "\n",
    "# confusion = confusion_matrix(y_test, y_pred)\n",
    "# print('Confusion Matrix\\n')\n",
    "# print(confusion)\n",
    "\n",
    "# precision = precision_score(y_test, y_pred, average='weighted')\n",
    "# recall = recall_score(y_test, y_pred, average='weighted')\n",
    "# f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "# print(f\"Precision: {precision:.4f}\")\n",
    "# print(f\"Recall: {recall:.4f}\")\n",
    "# print(f\"F1: {f1:.4f}\")\n",
    "\n",
    "# y_scores = classifier.predict_proba(X_test)[:, 1]\n",
    "# fpr, tpr, thresholds = roc_curve(y_test, y_scores)\n",
    "# auc = roc_auc_score(y_test, y_scores)\n",
    "\n",
    "# print(f\"AUC: {auc:.4f}\")\n",
    "\n",
    "# precision, recall, thresholds = precision_recall_curve(y_test, y_scores)\n",
    "# average_precision = average_precision_score(y_test, y_scores)\n",
    "\n",
    "# print(f\"Average Precision: {average_precision:.4f}\")\n",
    "\n",
    "# scores = cross_val_score(classifier, X, y, cv=5)\n",
    "# print(f\"Cross-Validation Accuracy Scores: {scores}\")\n",
    "\n",
    "# precision = precision_score(y_test, y_pred, average='weighted')\n",
    "# recall = recall_score(y_test, y_pred, average='weighted')\n",
    "# plt.xticks([0, 1, 2], ['Negative', 'Neutral', 'Positive'])\n",
    "# plt.yticks([0, 1, 2], ['Negative', 'Neutral', 'Positive'])\n",
    "# plt.colorbar()\n",
    "# plt.show()\n",
    "\n",
    "# # ROC Curve\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# plt.plot(fpr, tpr, label='ROC Curve (AUC = {:.4f})'.format(auc))\n",
    "# plt.plot([0, 1], [0, 1], 'k--')\n",
    "# plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "# plt.xlabel('False Positive Rate')\n",
    "# plt.ylabel('True Positive Rate')\n",
    "# plt.legend(loc='lower right')\n",
    "# plt.show()\n",
    "\n",
    "# # Precision-Recall Curve\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# plt.plot(recall, precision, label='Precision-Recall Curve (AP = {:.4f})'.format(average_precision))\n",
    "# plt.title('Precision-Recall Curve')\n",
    "# plt.xlabel('Recall')\n",
    "# plt.ylabel('Precision')\n",
    "# plt.legend(loc='lower left')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We display a tsne plot of the word vectors\n",
    "def tsne_plot(model):\n",
    "    vocab = model.wv.index_to_key\n",
    "\n",
    "    \"Creates and TSNE model and plots it\"\n",
    "    labels = []\n",
    "    tokens = []\n",
    "\n",
    "    for word in vocab:\n",
    "        tokens.append(model.wv[word])\n",
    "        labels.append(word)\n",
    "        #print(tokens)\n",
    "        #print(labels)\n",
    "    tokens = np.array(tokens)\n",
    "    tsne_model = TSNE(perplexity=20, n_components=2, init='pca', n_iter=2500, random_state=23)\n",
    "    new_values = tsne_model.fit_transform(tokens)\n",
    "\n",
    "\n",
    "    \n",
    "    x = []\n",
    "    y = []\n",
    "    for value in new_values:\n",
    "        x.append(value[0])\n",
    "        y.append(value[1])\n",
    "    \n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(16, 16)) \n",
    "    for i in range(len(x)):\n",
    "        plt.scatter(x[i],y[i])\n",
    "        plt.annotate(labels[i],\n",
    "                     xy=(x[i], y[i]),\n",
    "                     xytext=(5, 2),\n",
    "                     textcoords='offset points',\n",
    "                     ha='right',\n",
    "                     va='bottom')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "tsne_plot(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "com6: To look at the tensorboard results, you will need to download the VS tensorboard module or go to the corresponding port on your localhost (and making sure that the port is not used).\n",
    "Please find the analysis of the"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-21 15:35:47.098242: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:303] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "/Users/alexandrecogordan/miniconda3/envs/tensorflow/lib/python3.10/site-packages/tensorflow/python/client/session.py:1770: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n",
      "2024-01-21 15:35:47.098269: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:269] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'projections/model.ckpt-1434'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_name = \"word2vec.model\"\n",
    "model = gensim.models.keyedvectors.KeyedVectors.load(file_name)\n",
    "\n",
    "max_size = len(model.wv.index_to_key)-1\n",
    "\n",
    "w2v = np.zeros((max_size,model.vector_size))\n",
    "\n",
    "if not os.path.exists('projections'):\n",
    "    os.makedirs('projections')\n",
    "    \n",
    "with open(\"projections/metadata.tsv\", 'w+') as file_metadata:\n",
    "    \n",
    "    for i, word in enumerate(model.wv.index_to_key[:max_size]):\n",
    "        \n",
    "        #store the embeddings of the word\n",
    "        w2v[i] = model.wv[word]\n",
    "        \n",
    "        #write the word to a file \n",
    "        file_metadata.write(word + '\\n')\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "sess = tf.compat.v1.InteractiveSession()\n",
    "\n",
    "with tf.device(\"/cpu:0\"):\n",
    "    embedding = tf.Variable(w2v, trainable=False, name='embedding')\n",
    "\n",
    "sess.run(tf.compat.v1.global_variables_initializer())\n",
    "\n",
    "saver = tf.compat.v1.train.Saver()\n",
    "writer = tf.compat.v1.summary.FileWriter('projections', sess.graph)\n",
    "config = projector.ProjectorConfig()\n",
    "embed= config.embeddings.add()\n",
    "\n",
    "embed.tensor_name = 'embedding'\n",
    "embed.metadata_path = 'metadata.tsv'\n",
    "\n",
    "projector.visualize_embeddings(writer, config)\n",
    "saver.save(sess, 'projections/word2vec.ckpt', global_step=max_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wine',\n",
       " 'food',\n",
       " 'french',\n",
       " 'alliance',\n",
       " 'great',\n",
       " 'night',\n",
       " 'staff',\n",
       " 'restaurant',\n",
       " 'good',\n",
       " 'server']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def semantic_search(query_word, model, topn=10):\n",
    "    query_vector = model.wv[query_word]\n",
    "    all_words = model.wv.index_to_key\n",
    "\n",
    "    # Calculate cosine distance between query and all other words\n",
    "    distances = {word: cosine(query_vector, model.wv[word]) for word in all_words}\n",
    "    \n",
    "    # Sort words by distance (lower is more similar)\n",
    "    sorted_words = sorted(distances, key=distances.get)\n",
    "\n",
    "    # Return the topn closest words\n",
    "    return sorted_words[:topn]\n",
    "\n",
    "# Example usage\n",
    "search_results = semantic_search('wine', word2vec_model)\n",
    "\n",
    "search_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Should we add a tensorboard?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.96      0.98      0.97       102\n",
      "           2       0.89      0.95      0.92       102\n",
      "           3       0.93      0.84      0.88        85\n",
      "\n",
      "    accuracy                           0.93       289\n",
      "   macro avg       0.93      0.92      0.92       289\n",
      "weighted avg       0.93      0.93      0.93       289\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([3])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(generated_df['cleaned_text'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, generated_df['sentiment'], test_size=0.2, random_state=42)\n",
    "\n",
    "classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "def classify_review(review_text):\n",
    "    # Preprocess the review_text similar to your training data preprocessing\n",
    "    preprocessed_text = preprocessing(review_text)  # Implement this function based on your preprocessing steps\n",
    "    \n",
    "    # Transform the review text to TF-IDF features\n",
    "    tfidf_features = tfidf_vectorizer.transform([preprocessed_text])\n",
    "    \n",
    "    # Predict the label\n",
    "    predicted_label = classifier.predict(tfidf_features)\n",
    "    \n",
    "    return predicted_label\n",
    "\n",
    "classify_review(\"This is a great restaurant! I loved the food and the service was amazing.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['review_classification_pipeline.joblib']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "import joblib\n",
    "\n",
    "pipeline = make_pipeline(TfidfVectorizer(), RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "\n",
    "# Fit the pipeline with your data\n",
    "pipeline.fit(generated_df['cleaned_text'], generated_df['sentiment'])\n",
    "\n",
    "# Save the pipeline\n",
    "joblib.dump(pipeline, 'review_classification_pipeline.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-20 15:08:02.257800: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:303] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-01-20 15:08:02.257862: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:269] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "/Users/alexandrecogordan/miniconda3/envs/tensorflow/lib/python3.10/site-packages/tensorflow/python/client/session.py:1770: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'projections/tfidf.ckpt'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorboard.plugins import projector\n",
    "\n",
    "# Assuming X_tfidf is your TF-IDF matrix\n",
    "svd = TruncatedSVD(n_components=50, random_state=42)  # Reduce to 50 dimensions\n",
    "X_reduced = svd.fit_transform(X_tfidf)\n",
    "\n",
    "# Create a projection directory\n",
    "if not os.path.exists('projections'):\n",
    "    os.makedirs('projections')\n",
    "\n",
    "# Save the reduced embeddings and feature names\n",
    "with open(\"projections/metadata.tsv\", 'w+') as file_metadata:\n",
    "    # Use get_feature_names_out() for scikit-learn 0.24 and newer\n",
    "    for feature_name in tfidf_vectorizer.get_feature_names_out():\n",
    "        file_metadata.write(feature_name + '\\n')\n",
    "\n",
    "# TF-IDF embeddings after dimensionality reduction\n",
    "tfidf_embeddings = np.array(X_reduced)\n",
    "\n",
    "# Disable eager execution (needed for TensorBoard in TF1.x)\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "# Start an interactive session\n",
    "sess = tf.compat.v1.InteractiveSession()\n",
    "\n",
    "# Create a TensorFlow variable for the embeddings\n",
    "with tf.device(\"/cpu:0\"):\n",
    "    embedding_var = tf.Variable(tfidf_embeddings, trainable=False, name='tfidf_embedding')\n",
    "\n",
    "# Initialize the variable\n",
    "sess.run(tf.compat.v1.global_variables_initializer())\n",
    "\n",
    "# Save the embeddings in a checkpoint\n",
    "saver = tf.compat.v1.train.Saver([embedding_var])  # Add embedding_var to the saver\n",
    "writer = tf.compat.v1.summary.FileWriter('projections', sess.graph)\n",
    "\n",
    "# Setup the projector configuration\n",
    "config = projector.ProjectorConfig()\n",
    "embedding = config.embeddings.add()\n",
    "embedding.tensor_name = embedding_var.name\n",
    "embedding.metadata_path = 'metadata.tsv'\n",
    "\n",
    "# Visualize embeddings\n",
    "projector.visualize_embeddings(writer, config)\n",
    "saver.save(sess, 'projections/tfidf.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-17 16:33:50.937 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run /Users/alexandrecogordan/miniconda3/envs/tensorflow/lib/python3.10/site-packages/ipykernel_launcher.py [ARGUMENTS]\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "\n",
    "def resize_image(image_path, width, height):\n",
    "    image = Image.open(image_path)\n",
    "    resized_image = image.resize((width, height))\n",
    "    return resized_image\n",
    "\n",
    "st.set_page_config(page_title=\"Gastonomy\", page_icon=\"üçΩÔ∏è\", layout=\"wide\")\n",
    "    \n",
    "st.sidebar.markdown(\"Select a city and a restaurant to generate a review.\")\n",
    "city = st.sidebar.selectbox(\"City\", sorted(df['location'].unique()))\n",
    "\n",
    "# Dictionary mapping city names to image filenames\n",
    "city_images = {\n",
    "    'New Orleans': 'resources/new-orleans.jpg',\n",
    "    'New York City': 'resources/new-york.jpg',\n",
    "    'Chicago': 'resources/chicago.jpg',\n",
    "    'Los Angeles': 'resources/los-angeles.jpg',\n",
    "    'San Francisco': 'resources/san-francisco.jpg',\n",
    "    'Philadelphia': 'resources/philadelphia.jpg',\n",
    "    'Las Vegas': 'resources/las-vegas.jpg',\n",
    "    'Houston': 'resources/houston.jpg',\n",
    "    'Phoenix': 'resources/phoenix.jpg',\n",
    "    'Miami': 'resources/miami.jpg'\n",
    "}\n",
    "\n",
    "# Display image based on selected city\n",
    "if city in city_images:\n",
    "    image_filename = city_images[city]\n",
    "    resized_image = resize_image(image_filename, 1920, 1080)\n",
    "    st.image(resized_image, caption=city)\n",
    "else:\n",
    "    st.write(\"Image not found for selected city.\")\n",
    "\n",
    "st.title(\"Restaurant Review Analysis\")\n",
    "\n",
    "st.header(\"Quel sont les aspects les plus importants pour vous dans un restaurant?\")\n",
    "topics = st.multiselect(\"Choisissez vos aspects\", sorted(label_dict.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A faire: Faire choisir a l'utilisateur un ou des topics qu'il aimerait aborder. (S'il est interesse par le food quality par exemple on lui recommandera un restaurant avec une tres bonne food quality - peux-t-on recuperer les restaurants qui correspondent aux avis?).\n",
    "\n",
    "Montrer aussi les topics principaux les plus importants pour les resturants francais de chaque ville\n",
    "\n",
    "Later on, maybe use these topics to enhance the importance of them in summarised reviews.\n",
    "\n",
    "What I suggest is this: Topics principaux par villes. On demande a l'utilisateur de choisir ce qu'il prefere a travers un chatbot et ensuite on trouve les restaurants avec les meilleurs topics et de ce resturant on montre aussi ses meilleurs atouts avec les topics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
